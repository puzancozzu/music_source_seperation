{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7662342,"sourceType":"datasetVersion","datasetId":4467810},{"sourceId":7679335,"sourceType":"datasetVersion","datasetId":4469761}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_io as tfio\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport cv2\nfrom tensorflow.keras import layers\nfrom keras.models import Model, Sequential\nfrom tensorflow.keras.applications import VGG19\nfrom keras.layers import Dense, Conv2D, Flatten, BatchNormalization, LeakyReLU\nfrom keras.layers import Conv2DTranspose, Dropout, ReLU, Input, Concatenate, ZeroPadding2D\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom IPython.display import Audio\nimport librosa\nimport random\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-02T18:35:16.12725Z","iopub.execute_input":"2024-03-02T18:35:16.127588Z","iopub.status.idle":"2024-03-02T18:35:29.097307Z","shell.execute_reply.started":"2024-03-02T18:35:16.12756Z","shell.execute_reply":"2024-03-02T18:35:29.096488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) == 0:\n    print(\"No GPU devices found. Make sure your GPU is properly installed and configured.\")\nelse:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    print(\"GPU configured successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:29.098737Z","iopub.execute_input":"2024-03-02T18:35:29.099244Z","iopub.status.idle":"2024-03-02T18:35:29.275009Z","shell.execute_reply.started":"2024-03-02T18:35:29.099217Z","shell.execute_reply":"2024-03-02T18:35:29.273938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Check available GPUs\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    # Restrict TensorFlow to only allocate GPU memory growth\n    try:\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n        print(\"GPU memory growth set to True\")\n    except RuntimeError as e:\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:29.276267Z","iopub.execute_input":"2024-03-02T18:35:29.276641Z","iopub.status.idle":"2024-03-02T18:35:29.308121Z","shell.execute_reply.started":"2024-03-02T18:35:29.276606Z","shell.execute_reply":"2024-03-02T18:35:29.307049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Check if TensorFlow can access GPU\nnum_gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n\nif num_gpus > 0:\n    print(\"Num GPUs Available: \", num_gpus)\n    print(\"TensorFlow is using GPU.\")\nelse:\n    print(\"No GPU available. TensorFlow is using CPU.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:29.309983Z","iopub.execute_input":"2024-03-02T18:35:29.31027Z","iopub.status.idle":"2024-03-02T18:35:29.317623Z","shell.execute_reply.started":"2024-03-02T18:35:29.310246Z","shell.execute_reply":"2024-03-02T18:35:29.316772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load(audio_file):\n    audio = tf.io.read_file(audio_file)\n    audio , sr= tf.audio.decode_wav(audio)\n    audio = tf.squeeze(audio, axis=[-1])\n    audio = tf.cast(audio,tf.float32)\n    return audio\n\n\n# def load(audio_file):\n#     audio = tfio.audio.AudioIOTensor(audio_file).to_tensor()\n#     audio = tf.squeeze(audio, axis=[-1])\n#     audio = tf.cast(audio,tf.float32)/32768.0\n#     return audio","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:29.318778Z","iopub.execute_input":"2024-03-02T18:35:29.319111Z","iopub.status.idle":"2024-03-02T18:35:29.327452Z","shell.execute_reply.started":"2024-03-02T18:35:29.319082Z","shell.execute_reply":"2024-03-02T18:35:29.326632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio = load('/kaggle/input/music-for-gan/music/val/music_144.wav')\nprint(audio.shape, type(audio))\nAudio(audio.numpy().reshape([2646000]), rate=44100)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:29.32856Z","iopub.execute_input":"2024-03-02T18:35:29.329218Z","iopub.status.idle":"2024-03-02T18:35:29.748234Z","shell.execute_reply.started":"2024-03-02T18:35:29.329188Z","shell.execute_reply":"2024-03-02T18:35:29.746894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bansuri_audio = audio[:1323000]\nprint(bansuri_audio)\nAudio(bansuri_audio.numpy().reshape([1323000]), rate = 44100)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:29.749813Z","iopub.execute_input":"2024-03-02T18:35:29.750495Z","iopub.status.idle":"2024-03-02T18:35:29.858216Z","shell.execute_reply.started":"2024-03-02T18:35:29.750452Z","shell.execute_reply":"2024-03-02T18:35:29.856844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mixture_audio = audio[1323000:]\nAudio(mixture_audio.numpy().reshape([1323000]), rate=44100)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:29.859876Z","iopub.execute_input":"2024-03-02T18:35:29.860401Z","iopub.status.idle":"2024-03-02T18:35:29.925726Z","shell.execute_reply.started":"2024-03-02T18:35:29.860354Z","shell.execute_reply":"2024-03-02T18:35:29.924164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(bansuri_audio.shape, type(bansuri_audio))\nbansuri_spectogram = tfio.audio.spectrogram(bansuri_audio, nfft = 1022, window = 1022, stride=256)\nprint(bansuri_spectogram.shape)\nplt.figure()\nplt.imshow(tf.math.log(tf.transpose(bansuri_spectogram)).numpy())","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:29.927233Z","iopub.execute_input":"2024-03-02T18:35:29.927918Z","iopub.status.idle":"2024-03-02T18:35:30.942756Z","shell.execute_reply.started":"2024-03-02T18:35:29.927882Z","shell.execute_reply":"2024-03-02T18:35:30.941849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(audio.shape)\nmixture_spectogram = tfio.audio.spectrogram(mixture_audio, nfft = 1022, window = 1022, stride=256)\nprint(mixture_spectogram.shape)\nplt.figure()\nplt.imshow(tf.math.log(tf.transpose(mixture_spectogram)).numpy())","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:30.946202Z","iopub.execute_input":"2024-03-02T18:35:30.947005Z","iopub.status.idle":"2024-03-02T18:35:31.364932Z","shell.execute_reply.started":"2024-03-02T18:35:30.946976Z","shell.execute_reply":"2024-03-02T18:35:31.364048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed1 = random.randint(0,2500)\nseed2 = random.randint(0,2500)\nbansuri_spectogram = tf.image.stateless_random_crop(bansuri_spectogram, (256,512), (seed1, seed2))\nplt.figure()\nplt.imshow(tf.math.log(tf.transpose(bansuri_spectogram)).numpy())\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:31.366271Z","iopub.execute_input":"2024-03-02T18:35:31.367039Z","iopub.status.idle":"2024-03-02T18:35:31.592165Z","shell.execute_reply.started":"2024-03-02T18:35:31.367005Z","shell.execute_reply":"2024-03-02T18:35:31.591318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_train_audio(bansuri_audio_path):\n    mixture = load(bansuri_audio_path)\n    bansuri = mixture[:88200]\n    mixture = mixture[88200:2*88200]\n    bansuri = tfio.audio.spectrogram(bansuri, nfft = 1022, window = 1022, stride=256)\n    mixture = tfio.audio.spectrogram(mixture, nfft = 1022, window = 1022, stride=256)\n    seed1 = random.randint(0,2500)\n    seed2 = random.randint(0,2500)\n    bansuri = tf.image.stateless_random_crop(bansuri, (256,512), (seed1, seed2))\n    mixture = tf.image.stateless_random_crop(mixture, (256,512), (seed1, seed2))\n    bansuri = tf.reshape(bansuri,(256,512,1))\n    mixture = tf.reshape(mixture,(256,512,1))\n    return mixture, bansuri","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:31.593378Z","iopub.execute_input":"2024-03-02T18:35:31.593727Z","iopub.status.idle":"2024-03-02T18:35:31.602481Z","shell.execute_reply.started":"2024-03-02T18:35:31.593697Z","shell.execute_reply":"2024-03-02T18:35:31.601495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:31.603725Z","iopub.execute_input":"2024-03-02T18:35:31.604001Z","iopub.status.idle":"2024-03-02T18:35:31.613076Z","shell.execute_reply.started":"2024-03-02T18:35:31.603971Z","shell.execute_reply":"2024-03-02T18:35:31.612281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create input pipeline\ntrain_dataset = tf.data.Dataset.list_files('/kaggle/input/music-gan2/music2sec/kaggle/working/music/train/*.wav')\ntrain_dataset = train_dataset.map(load_train_audio)\ntrain_dataset = train_dataset.shuffle(10).batch(BATCH_SIZE)\ntrain_dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:31.614028Z","iopub.execute_input":"2024-03-02T18:35:31.614276Z","iopub.status.idle":"2024-03-02T18:35:33.467661Z","shell.execute_reply.started":"2024-03-02T18:35:31.614254Z","shell.execute_reply":"2024-03-02T18:35:33.466699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_dataset = tf.data.Dataset.list_files('/kaggle/input/music-gan2/music2sec/kaggle/working/music/val/*.wav')\nvalidation_dataset = validation_dataset.map(load_train_audio)\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE)\nvalidation_dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:33.468874Z","iopub.execute_input":"2024-03-02T18:35:33.469219Z","iopub.status.idle":"2024-03-02T18:35:33.706947Z","shell.execute_reply.started":"2024-03-02T18:35:33.469187Z","shell.execute_reply":"2024-03-02T18:35:33.706009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import Input, Model\nfrom keras.layers import Conv2D, Dropout, BatchNormalization, LeakyReLU, Conv2DTranspose, Activation, Concatenate, Multiply\nfrom tensorflow.keras.utils import plot_model\n\ndef unet(inputs=Input((256, 512, 1))):\n    conv1 = Conv2D(64, 5, strides=2, padding='same')(inputs)\n    conv1 = BatchNormalization(axis=-1)(conv1)\n    conv1 = LeakyReLU(alpha=0.2)(conv1)\n\n    conv2 = Conv2D(128, 5, strides=2, padding='same')(conv1)\n    conv2 = BatchNormalization(axis=-1)(conv2)\n    conv2 = LeakyReLU(alpha=0.2)(conv2)\n\n    conv3 = Conv2D(256, 5, strides=2, padding='same')(conv2)\n    conv3 = BatchNormalization(axis=-1)(conv3)\n    conv3 = LeakyReLU(alpha=0.2)(conv3)\n\n    conv4 = Conv2D(512, 5, strides=2, padding='same')(conv3)\n    conv4 = BatchNormalization(axis=-1)(conv4)\n    conv4 = LeakyReLU(alpha=0.2)(conv4)\n\n    conv5 = Conv2D(1024, 5, strides=2, padding='same')(conv4)\n    conv5 = BatchNormalization(axis=-1)(conv5)\n    conv5 = LeakyReLU(alpha=0.2)(conv5)\n\n    conv6 = Conv2D(1024, 5, strides=2, padding='same')(conv5)\n    conv6 = BatchNormalization(axis=-1)(conv6)\n    conv6 = LeakyReLU(alpha=0.2)(conv6)\n\n    deconv7 = Conv2DTranspose(1024, 5, strides=2, padding='same')(conv6)\n    deconv7 = BatchNormalization(axis=-1)(deconv7)\n    deconv7 = Dropout(0.5)(deconv7)\n    deconv7 = Activation('relu')(deconv7)\n\n    deconv8 = Concatenate(axis=-1)([deconv7, conv5])\n    deconv8 = Conv2DTranspose(512, 5, strides=2, padding='same')(deconv8)\n    deconv8 = BatchNormalization(axis=-1)(deconv8)\n    deconv8 = Dropout(0.5)(deconv8)\n    deconv8 = Activation('relu')(deconv8)\n\n    deconv9 = Concatenate(axis=-1)([deconv8, conv4])\n    deconv9 = Conv2DTranspose(256, 5, strides=2, padding='same')(deconv9)\n    deconv9 = BatchNormalization(axis=-1)(deconv9)\n    deconv9 = Dropout(0.5)(deconv9)\n    deconv9 = Activation('relu')(deconv9)\n\n    deconv10 = Concatenate(axis=-1)([deconv9, conv3])\n    deconv10 = Conv2DTranspose(128, 5, strides=2, padding='same')(deconv10)\n    deconv10 = BatchNormalization(axis=-1)(deconv10)\n    deconv10 = Activation('relu')(deconv10)\n\n    deconv11 = Concatenate(axis=-1)([deconv10, conv2])\n    deconv11 = Conv2DTranspose(64, 5, strides=2, padding='same')(deconv11)\n    deconv11 = BatchNormalization(axis=-1)(deconv11)\n    deconv11 = Activation('relu')(deconv11)\n\n    deconv12 = Concatenate(axis=-1)([deconv11, conv1])\n    deconv12 = Conv2DTranspose(1, 5, strides=2, padding='same')(deconv12)\n    deconv12 = Activation('relu')(deconv12)\n    deconv12 = BatchNormalization(axis=-1)(deconv12)\n\n    output = Multiply()([deconv12, inputs])\n    return Model(inputs=inputs, outputs=output)\n\n\n# if __name__ == '__main__':\ninputs = Input((256, 512, 1))\ngen = unet(inputs)\ngen.summary()\nplot_model(gen, to_file='/kaggle/working/model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:33.708158Z","iopub.execute_input":"2024-03-02T18:35:33.708443Z","iopub.status.idle":"2024-03-02T18:35:35.027783Z","shell.execute_reply.started":"2024-03-02T18:35:33.708419Z","shell.execute_reply":"2024-03-02T18:35:35.026941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# downsample block\ndef downsample(filters, size,dropout=False, batchnorm = True):\n    init = tf.random_normal_initializer(0.,0.02)\n    result = Sequential()\n    result.add(Conv2D(filters, size, strides = 2, padding = \"same\", kernel_initializer = init, use_bias = False))\n    if batchnorm == True:\n        result.add(BatchNormalization())\n    if dropout == True :\n        result.add(Dropout(0.3))\n    result.add(LeakyReLU())\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.02903Z","iopub.execute_input":"2024-03-02T18:35:35.029362Z","iopub.status.idle":"2024-03-02T18:35:35.036402Z","shell.execute_reply.started":"2024-03-02T18:35:35.02932Z","shell.execute_reply":"2024-03-02T18:35:35.03549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator():\n    init = tf.random_normal_initializer(0., 0.02)\n\n    inp = Input(shape = [256, 512, 1], name = \"mixture\")\n    tar = Input(shape = [256, 512, 1], name = \"seperated\")\n    x = Concatenate()([inp, tar])\n    down1 = downsample(32,4,False)(x)\n    down2 = downsample(62, 4)(down1)\n    down3 = downsample(128, 4)(down2)\n\n    zero_pad1 = ZeroPadding2D()(down3)\n    conv = Conv2D(256, 4, strides = 1, kernel_initializer = init, use_bias = False)(zero_pad1)\n    leaky_relu = LeakyReLU()(conv)\n    zero_pad2 = ZeroPadding2D()(leaky_relu)\n    last = Conv2D(1, 4, strides = 1, kernel_initializer=init)(zero_pad2)\n    return Model(inputs = [inp, tar], outputs = last)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.037626Z","iopub.execute_input":"2024-03-02T18:35:35.037918Z","iopub.status.idle":"2024-03-02T18:35:35.049152Z","shell.execute_reply.started":"2024-03-02T18:35:35.037892Z","shell.execute_reply":"2024-03-02T18:35:35.048276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disc = discriminator()\ndisc.summary()\nplot_model(disc, show_shapes=True, dpi = 64)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.050399Z","iopub.execute_input":"2024-03-02T18:35:35.0508Z","iopub.status.idle":"2024-03-02T18:35:35.328309Z","shell.execute_reply.started":"2024-03-02T18:35:35.050769Z","shell.execute_reply":"2024-03-02T18:35:35.32743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.losses import BinaryCrossentropy\nloss_function = BinaryCrossentropy(from_logits=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.329612Z","iopub.execute_input":"2024-03-02T18:35:35.330132Z","iopub.status.idle":"2024-03-02T18:35:35.334654Z","shell.execute_reply.started":"2024-03-02T18:35:35.3301Z","shell.execute_reply":"2024-03-02T18:35:35.33373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generator_loss(disc_generated_output,input_, gen_output, target):\n    gan_loss = loss_function(tf.ones_like(disc_generated_output), disc_generated_output)\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n    total_gen_loss = gan_loss + 100 * l1_loss \n    return total_gen_loss, gan_loss, l1_loss","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.335784Z","iopub.execute_input":"2024-03-02T18:35:35.336095Z","iopub.status.idle":"2024-03-02T18:35:35.345253Z","shell.execute_reply.started":"2024-03-02T18:35:35.336067Z","shell.execute_reply":"2024-03-02T18:35:35.34458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = loss_function(tf.ones_like(disc_real_output), disc_real_output)\n    generated_loss = loss_function(tf.zeros_like(disc_generated_output), disc_generated_output)\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss, real_loss, generated_loss","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.34624Z","iopub.execute_input":"2024-03-02T18:35:35.346517Z","iopub.status.idle":"2024-03-02T18:35:35.35608Z","shell.execute_reply.started":"2024-03-02T18:35:35.346494Z","shell.execute_reply":"2024-03-02T18:35:35.355239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_optimizer = Adam(lr= 5e-4, beta_1=0.5)\ndiscriminator_optimizer = Adam(lr = 5e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.356978Z","iopub.execute_input":"2024-03-02T18:35:35.357217Z","iopub.status.idle":"2024-03-02T18:35:35.372619Z","shell.execute_reply.started":"2024-03-02T18:35:35.357196Z","shell.execute_reply":"2024-03-02T18:35:35.371907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(mixture, target, epoch, training_discriminator):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = gen(mixture, training=True)\n        disc_real_output = disc([mixture, target], training=True)\n        disc_generated_output = disc([mixture, gen_output], training=True)\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output,mixture, gen_output, target)\n        disc_loss, disc_real_loss, disc_generated_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n        if epoch > 8:\n            if training_discriminator:\n                discriminator_gradients = disc_tape.gradient(disc_loss, disc.trainable_variables)\n                discriminator_optimizer.apply_gradients(zip(discriminator_gradients, disc.trainable_variables))\n            else:\n                generator_gradients = gen_tape.gradient(gen_total_loss, gen.trainable_variables)\n                generator_optimizer.apply_gradients(zip(generator_gradients, gen.trainable_variables))\n        else:\n            generator_gradients = gen_tape.gradient(gen_total_loss, gen.trainable_variables)\n            discriminator_gradients = disc_tape.gradient(disc_loss, disc.trainable_variables)\n            generator_optimizer.apply_gradients(zip(generator_gradients, gen.trainable_variables))\n            discriminator_optimizer.apply_gradients(zip(discriminator_gradients, disc.trainable_variables))\n\n        return gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss,disc_real_loss, disc_generated_loss","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.373689Z","iopub.execute_input":"2024-03-02T18:35:35.373958Z","iopub.status.idle":"2024-03-02T18:35:35.383351Z","shell.execute_reply.started":"2024-03-02T18:35:35.373935Z","shell.execute_reply":"2024-03-02T18:35:35.382432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef validation_step(mixture, target):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = gen(mixture, training=False)\n        disc_real_output = disc([mixture, target], training=False)\n        disc_generated_output = disc([mixture, gen_output], training=False)\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output,mixture, gen_output, target)\n        disc_loss, disc_real_loss, disc_generated_loss = discriminator_loss(disc_real_output, disc_generated_output)\n        return gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss,disc_real_loss, disc_generated_loss","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.384322Z","iopub.execute_input":"2024-03-02T18:35:35.384886Z","iopub.status.idle":"2024-03-02T18:35:35.397389Z","shell.execute_reply.started":"2024-03-02T18:35:35.384863Z","shell.execute_reply":"2024-03-02T18:35:35.396562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"each_epoch_train_generator_losses = []\neach_epoch_train_generator_gan_losses = []\neach_epoch_train_generator_l1_losses = []\neach_epoch_train_discriminator_losses = []\neach_epoch_train_discriminator_real_losses = []\neach_epoch_train_discriminator_generated_losses = []\neach_epoch_test_generator_losses = []\neach_epoch_test_generator_gan_losses = []\neach_epoch_test_generator_l1_losses = []\neach_epoch_test_discriminator_losses = []\neach_epoch_test_discriminator_real_losses = []\neach_epoch_test_discriminator_generated_losses = []\ntrain_generator_losses = []\ntrain_generator_gan_losses = []\ntrain_generator_l1_losses = []\ntrain_discriminator_losses = []\ntrain_discriminator_real_losses = []\ntrain_discriminator_generated_losses = []\ntest_generator_losses = []\ntest_generator_gan_losses = []\ntest_generator_l1_losses = []\ntest_discriminator_losses = []\ntest_discriminator_real_losses = []\ntest_discriminator_generated_losses =[]\neach_epoch_all_records = []\nall_records = []\ndef fit(train_ds, epochs, test_ds, gen_model):\n    global each_epoch_train_generator_losses, each_epoch_train_generator_gan_losses, each_epoch_train_generator_l1_losses, each_epoch_train_discriminator_losses,each_epoch_train_discriminator_real_losses, each_epoch_train_discriminator_generated_losses\n    global each_epoch_test_generator_losses, each_epoch_test_generator_gan_losses, each_epoch_test_generator_l1_losses, each_epoch_test_discriminator_losses, each_epoch_test_discriminator_real_losses, each_epoch_test_discriminator_generated_losses\n    global train_generator_losses, train_generator_gan_losses, train_generator_l1_losses,train_discriminator_real_losses, train_discriminator_generated_losses, test_generator_losses, test_generator_gan_losses, test_generator_l1_losses,test_discriminator_losses, test_discriminator_real_losses, test_discriminator_generated_losses\n    global all_records, each_epoch_all_records\n    \n    # Initialize counters and flags\n    consecutive_epochs_high_loss = 0\n    training_discriminator = True\n    stop_training = False\n    min_learning_rate = 1e-7  # Set your desired minimum learning rate\n    time_start = time.time()\n    \n    for ep in range(epochs):\n        epoch = ep\n        start = time.time()\n        train_count = 0\n        test_count = 0\n\n        print(f\"Epoch {epoch}\")\n\n        for n, (input_, target) in train_ds.enumerate():\n            if epoch > 10:\n                train_gen_loss, train_gen_gan_loss, train_gen_l1_loss, train_disc_loss, train_disc_real_loss, train_disc_generated_loss = train_step(input_, target, epoch, training_discriminator=training_discriminator)\n\n            else:\n                # If epoch is less than or equal to 10, train both generator and discriminator\n                train_gen_loss, train_gen_gan_loss, train_gen_l1_loss, train_disc_loss, train_disc_real_loss, train_disc_generated_loss  = train_step(input_, target, epoch, training_discriminator=None)\n\n            each_epoch_train_generator_losses.append(train_gen_loss)\n            each_epoch_train_generator_gan_losses.append(train_gen_gan_loss)\n            each_epoch_train_generator_l1_losses.append(train_gen_l1_loss)\n#             each_epoch_train_generator_l2_losses.append(train_gen_l2_loss)\n#             each_epoch_train_generator_perceptual_losses.append(train_gen_perceptual_loss)\n            each_epoch_train_discriminator_losses.append(train_disc_loss)\n            each_epoch_train_discriminator_real_losses.append(train_disc_real_loss)\n            each_epoch_train_discriminator_generated_losses.append(train_disc_generated_loss)\n#             each_epoch_train_ssim.append(tf.reduce_mean(train_ssim.numpy()))\n#             each_epoch_train_psnr.append(tf.reduce_mean(train_psnr.numpy()))\n#             each_epoch_train_lpips.append(tf.reduce_mean(train_lpips.numpy()))\n#             print(train_count)\n            train_count += 1\n#             print(n)\n        #each_epoch_train_generator_losses = np.array(each_epoch_train_generator_losses)\n        #print(each_epoch_train_discriminator_losses.shape())\n        #print(each_epoch_train_generator_losses.shape())\n        #print(each_epoch_train_ssim())\n        print(\"Training Details\")\n        print(\"Generator-- total_loss:{:.5f} gan_loss:{:.5f} l1_loss:{:.5f} Discriminator-- total_loss:{:.5f} real_loss:{:.5f} generated_loss:{:.5f}\".format(np.mean(each_epoch_train_generator_losses),np.mean(each_epoch_train_generator_gan_losses), np.mean(each_epoch_train_generator_l1_losses), np.mean(each_epoch_train_discriminator_losses), np.mean(each_epoch_train_discriminator_generated_losses), np.mean(each_epoch_train_discriminator_real_losses)))\n        print(\"Time taken for epoch {} is {} sec\".format(epoch + 1, time.time() - start))\n        print(f\"Number of iteration {train_count}\")\n\n        train_generator_losses.append(np.mean(each_epoch_train_generator_losses))\n        train_generator_gan_losses.append(np.mean(each_epoch_train_generator_gan_losses))\n        train_generator_l1_losses.append(np.mean(each_epoch_train_generator_l1_losses))\n#         train_generator_l2_losses.append(np.mean(each_epoch_train_generator_l2_losses))\n#         train_generator_perceptual_losses.append(np.mean(each_epoch_train_generator_perceptual_losses))\n        train_discriminator_losses.append(np.mean(each_epoch_train_discriminator_losses))\n        train_discriminator_real_losses.append(np.mean(each_epoch_train_discriminator_real_losses))\n        train_discriminator_generated_losses.append(np.mean(each_epoch_train_discriminator_generated_losses))\n#         training_ssim.append(np.mean(each_epoch_train_ssim))\n#         training_psnr.append(np.mean(each_epoch_train_psnr))\n#         training_lpips.append(np.mean(each_epoch_train_lpips))\n\n        # Test\n        for n, (input_, target) in test_ds.enumerate():\n            test_gen_loss, test_gen_gan_loss, test_gen_l1_loss, test_disc_loss, test_disc_real_loss, test_disc_generated_loss, = validation_step(input_, target)\n            test_count += 1\n\n            each_epoch_test_generator_losses.append(test_gen_loss)\n            each_epoch_test_generator_gan_losses.append(test_gen_gan_loss)\n            each_epoch_test_generator_l1_losses.append(test_gen_l1_loss)\n#             each_epoch_test_generator_l2_losses.append(test_gen_l2_loss)\n#             each_epoch_test_generator_perceptual_losses.append(test_gen_perceptual_loss)\n            each_epoch_test_discriminator_losses.append(test_disc_loss)\n            each_epoch_test_discriminator_real_losses.append(test_disc_real_loss)\n            each_epoch_test_discriminator_generated_losses.append(test_disc_generated_loss)\n#             each_epoch_test_ssim.append(tf.reduce_mean(test_ssim.numpy()))\n#             each_epoch_test_psnr.append(tf.reduce_mean(test_psnr.numpy()))\n#             each_epoch_test_lpips.append(tf.reduce_mean(test_lpips.numpy()))\n    \n        print(\"Validation Details\")\n        print(\"Generator-- total_loss:{:.5f} gan_loss:{:.5f} l1_loss:{:.5f}  Discriminator-- total_loss:{:.5f} real_loss:{:.5f} generated_loss:{:.5f}\".format(np.mean(each_epoch_test_generator_losses),np.mean(each_epoch_test_generator_gan_losses), np.mean(each_epoch_test_generator_l1_losses),np.mean(each_epoch_test_discriminator_losses), np.mean(each_epoch_test_discriminator_generated_losses), np.mean(each_epoch_test_discriminator_real_losses)))\n        print(\"Time taken for epoch {} is {} sec\".format(epoch+1, time.time() - start))\n        print(f\"Number of iteration {test_count}\")\n#         LAMBDA = LAMBDA * 1.006\n        test_generator_losses.append(np.mean(each_epoch_test_generator_losses))\n        test_generator_gan_losses.append(np.mean(each_epoch_test_generator_gan_losses))\n        test_generator_l1_losses.append(np.mean(each_epoch_test_generator_l1_losses))\n#         test_generator_l2_losses.append(np.mean(each_epoch_test_generator_l2_losses))\n#         test_generator_perceptual_losses.append(np.mean(each_epoch_test_generator_perceptual_losses))\n        test_discriminator_losses.append(np.mean(each_epoch_test_discriminator_losses))\n        test_discriminator_real_losses.append(np.mean(each_epoch_test_discriminator_real_losses))\n        test_discriminator_generated_losses.append(np.mean(each_epoch_test_discriminator_generated_losses))\n#         testing_ssim.append(np.mean(each_epoch_test_ssim))\n#         testing_psnr.append(np.mean(each_epoch_test_psnr))\n#         testing_lpips.append(np.mean(each_epoch_test_lpips))\n\n        each_epoch_all_records.append([np.mean(each_epoch_train_generator_losses),np.mean(each_epoch_train_generator_gan_losses), np.mean(each_epoch_train_generator_l1_losses), np.mean(each_epoch_train_discriminator_losses), np.mean(each_epoch_train_discriminator_generated_losses), np.mean(each_epoch_train_discriminator_real_losses), np.mean(each_epoch_test_generator_losses),np.mean(each_epoch_test_generator_gan_losses), np.mean(each_epoch_test_generator_l1_losses),np.mean(each_epoch_test_discriminator_losses), np.mean(each_epoch_test_discriminator_generated_losses), np.mean(each_epoch_test_discriminator_real_losses)])\n        all_records.append(each_epoch_all_records)\n        # emptying for next epoch\n        each_epoch_test_ssim = []\n        each_epoch_test_generator_losses = []\n        each_epoch_test_generator_gan_losses = []\n        each_epoch_test_generator_l1_losses = []\n#         each_epoch_test_generator_l2_losses = []\n#         each_epoch_test_generator_perceptual_losses = []\n        each_epoch_test_discriminator_losses = []\n        each_epoch_test_discriminator_real_losses = []\n        each_epoch_test_discriminator_generated_losses = []\n#         each_epoch_test_psnr = []\n#         each_epoch_test_lpips = []\n#         each_epoch_all_records = []\n        #emptyling list for next iteration\n        each_epoch_train_generator_losses = []\n        each_epoch_train_generator_gan_losses = []\n        each_epoch_train_generator_l1_losses = []\n#         each_epoch_train_generator_l2_losses = []\n#         each_epoch_train_generator_perceptual_losses = []\n        each_epoch_train_discriminator_losses = []\n        each_epoch_train_discriminator_real_losses = []\n        each_epoch_train_discriminator_generated_losses = []\n#         each_epoch_train_ssim = []\n#         each_epoch_train_psnr = []\n#         each_epoch_train_lpips = []\n        if time.time() - time_start > 41000:\n            stop_training = True\n#         if epoch == 0 or epoch % 3 == 0:\n#             # Function to display one random test image\n#             # def display_random_test_image(images, labels, gen_model):\n#             #     # Randomly select an index\n#             #     random_test_image_idx = random.randint(0, len(images) - 1)\n\n#             #     # Extract input and target from the selected index\n#             #     random_test_input = images[random_test_image_idx]\n#             #     random_test_target = labels[random_test_image_idx]\n\n#             #     # Display the image\n#             #     display_one_random_test_image(random_test_input, random_test_target, gen_model)\n\n#             # Example usage\n#             display_random_test_image(gen, '/kaggle/input/imagenet/imagenet/val/ILSVRC2012_val_00000081.JPEG', epoch+1)\n        # Check if the difference between two consecutive epochs for the last 5 epochs is less than 1\n        if ep > 10:\n            # Check 1f the difference between two consecutive epochs for the last S epochs is less than 1 or 0.3\n            last_epochs_losses = train_discriminator_losses[-5:] if training_discriminator else train_generator_losses[-5:]\n            if all(((last_epochs_losses[i] - last_epochs_losses[i - 1])) < 0.01 if training_discriminator else ((last_epochs_losses[1] - last_epochs_losses[i - 1])) < 0.05 for i in range(1, 5)):\n                consecutive_epochs_high_loss += 1\n            else:\n                consecutive_epochs_high_loss = 0\n        \n        \n#         LAMBDA = LAMBDA*1.003\n            # If the condition is met for 4 consecutive epochs, reduce the learning rate\n        if consecutive_epochs_high_loss == 4:\n            if training_discriminator:\n                current_lr = generator_optimizer. learning_rate.numpy()\n                new_lr = max (current_lr * 0.45, min_learning_rate)\n                if new_lr > min_learning_rate:\n                    generator_optimizer.learning_rate.assign(new_lr)\n                    print (f\"Reduced generator learning rate to {new_lr} at epoch {epoch + 1}\")\n                    # Switch training focus\n                    training_discriminator = not training_discriminator\n                else:\n                    print(f\"Generator learning rate already at the minimum. Stopping training.\")\n                    stop_training = True\n            else:\n                current_lr = discriminator_optimizer.learning_rate.numpy()\n                new_lr = max (current_lr * 0.3, min_learning_rate)\n                if new_lr > min_learning_rate:\n                    discriminator_optimizer.learning_rate.assign(new_lr)\n                    print(f\"Reduced discriminator learning rate to {new_lr} at epoch {epoch + 1}.\")\n                    # Switch training focus\n                    training_discriminator = not training_discriminator\n                else:\n                    print(f\"Discriminator learning rate already at the minimum. Stopping training.\")\n                    stop_training = True\n\n                consecutive_epochs_high_loss = 0\n        # Check the flag variable\n        if stop_training:\n            break\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.398656Z","iopub.execute_input":"2024-03-02T18:35:35.398916Z","iopub.status.idle":"2024-03-02T18:35:35.434445Z","shell.execute_reply.started":"2024-03-02T18:35:35.398887Z","shell.execute_reply":"2024-03-02T18:35:35.433678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit(train_dataset, 350, validation_dataset,gen)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:35:35.435462Z","iopub.execute_input":"2024-03-02T18:35:35.435758Z","iopub.status.idle":"2024-03-02T18:42:52.358713Z","shell.execute_reply.started":"2024-03-02T18:35:35.435736Z","shell.execute_reply":"2024-03-02T18:42:52.355454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(train_generator_losses) + 1)\n\nplt.plot(epochs, train_generator_losses, '--b', label='Generator Loss')\nplt.plot(epochs, train_discriminator_losses, '-.r', label='Discriminator Loss')\nplt.title('Generator and Discriminator Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('/kaggle/working/train_loss_plot_1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:42:58.796386Z","iopub.execute_input":"2024-03-02T18:42:58.797372Z","iopub.status.idle":"2024-03-02T18:42:59.157684Z","shell.execute_reply.started":"2024-03-02T18:42:58.79733Z","shell.execute_reply":"2024-03-02T18:42:59.156732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(train_generator_losses) + 1)\n\nplt.plot(epochs, train_generator_gan_losses, '--b', label='Generator GAN Loss')\nplt.plot(epochs, train_discriminator_losses, '-.r', label='Discriminator Loss')\nplt.title('Generator GAN and Discriminator Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('/kaggle/working/train_loss_plot_1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:42:59.930996Z","iopub.execute_input":"2024-03-02T18:42:59.931859Z","iopub.status.idle":"2024-03-02T18:43:00.292226Z","shell.execute_reply.started":"2024-03-02T18:42:59.931823Z","shell.execute_reply":"2024-03-02T18:43:00.291256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(test_generator_losses) + 1)\n\nplt.plot(epochs, test_generator_losses, '--b', label='Generator Loss')\nplt.plot(epochs, test_discriminator_losses, '-.r', label='Discriminator Loss')\nplt.title('Generator and Discriminator Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('/kaggle/working/test_loss_plot_1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:01.101709Z","iopub.execute_input":"2024-03-02T18:43:01.102125Z","iopub.status.idle":"2024-03-02T18:43:01.434082Z","shell.execute_reply.started":"2024-03-02T18:43:01.102095Z","shell.execute_reply":"2024-03-02T18:43:01.433168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(train_generator_losses) + 1)\n\nplt.plot(epochs, train_generator_gan_losses, '--b', label='Train_Generator_GAN_Loss')\nplt.plot(epochs, test_generator_gan_losses, '-.r', label='Validation_Generator_GAN_Loss')\nplt.title('Generator GAN Loss for Training and Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('/kaggle/working/Gen_GAN_plot_1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:02.123973Z","iopub.execute_input":"2024-03-02T18:43:02.124324Z","iopub.status.idle":"2024-03-02T18:43:02.495808Z","shell.execute_reply.started":"2024-03-02T18:43:02.124297Z","shell.execute_reply":"2024-03-02T18:43:02.494961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(train_generator_losses) + 1)\n\nplt.plot(epochs, train_generator_l1_losses, '--b', label='Train_Generator_L1_Loss')\nplt.plot(epochs, test_generator_l1_losses, '-.r', label='Validation_Generator_L1_Loss')\nplt.title('Generator L1 Loss for Training and Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('/kaggle/working/Gen_L1_plot_1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:03.012275Z","iopub.execute_input":"2024-03-02T18:43:03.012663Z","iopub.status.idle":"2024-03-02T18:43:03.358387Z","shell.execute_reply.started":"2024-03-02T18:43:03.012633Z","shell.execute_reply":"2024-03-02T18:43:03.357425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(train_generator_losses) + 1)\n\nplt.plot(epochs, train_generator_losses, '--b', label='Train_Generator_Loss')\nplt.plot(epochs, test_generator_losses, '-.r', label='Validation_Generator_Loss')\nplt.title('Generator Loss for Training and Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('/kaggle/working/Gen_plot_1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:03.967513Z","iopub.execute_input":"2024-03-02T18:43:03.968461Z","iopub.status.idle":"2024-03-02T18:43:04.299512Z","shell.execute_reply.started":"2024-03-02T18:43:03.968427Z","shell.execute_reply":"2024-03-02T18:43:04.298566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(train_generator_losses) + 1)\n\nplt.plot(epochs, train_discriminator_losses, '--b', label='Train_Discriminator_Loss')\nplt.plot(epochs, test_discriminator_losses, '-.r', label='Validation_Discriminator_Loss')\nplt.title('Discriminator Loss for Training and Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('/kaggle/working/Disc_plot_1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:04.300937Z","iopub.execute_input":"2024-03-02T18:43:04.301199Z","iopub.status.idle":"2024-03-02T18:43:04.6208Z","shell.execute_reply.started":"2024-03-02T18:43:04.301176Z","shell.execute_reply":"2024-03-02T18:43:04.619868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(train_generator_losses) + 1)\n\nplt.plot(epochs, train_discriminator_real_losses, '--b', label='Train_Discriminator_Real_Loss')\nplt.plot(epochs, test_discriminator_real_losses, '-.r', label='Validation_Discriminator_Real_Loss')\nplt.title('Discriminator Real Loss for Training and Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('/kaggle/working/Disc_Real_plot_1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:04.762334Z","iopub.execute_input":"2024-03-02T18:43:04.763399Z","iopub.status.idle":"2024-03-02T18:43:05.09268Z","shell.execute_reply.started":"2024-03-02T18:43:04.763359Z","shell.execute_reply":"2024-03-02T18:43:05.091757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(train_generator_losses) + 1)\n\nplt.plot(epochs, train_discriminator_generated_losses, '--b', label='Train_Discriminator_Generated_Loss')\nplt.plot(epochs, test_discriminator_generated_losses, '-.r', label='Validation_Discriminator_Generated_Loss')\nplt.title('Discriminator Generated Loss for Training and Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('/kaggle/working/Disc_Generated_plot_1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:05.141023Z","iopub.execute_input":"2024-03-02T18:43:05.141548Z","iopub.status.idle":"2024-03-02T18:43:05.50783Z","shell.execute_reply.started":"2024-03-02T18:43:05.141502Z","shell.execute_reply":"2024-03-02T18:43:05.506919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nwith open('/kaggle/working/dataJan20.csv','w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerows(all_records)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:05.781256Z","iopub.execute_input":"2024-03-02T18:43:05.781735Z","iopub.status.idle":"2024-03-02T18:43:05.78967Z","shell.execute_reply.started":"2024-03-02T18:43:05.781695Z","shell.execute_reply":"2024-03-02T18:43:05.788682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('/kaggle/working/modelsFeb20')","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:06.306657Z","iopub.execute_input":"2024-03-02T18:43:06.307005Z","iopub.status.idle":"2024-03-02T18:43:06.311763Z","shell.execute_reply.started":"2024-03-02T18:43:06.306976Z","shell.execute_reply":"2024-03-02T18:43:06.310826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen.save('/kaggle/working/modelsFeb20/gen.h5')\ngen.save_weights('/kaggle/working/modelsFeb20/gen_weight.keras')\ndisc.save('/kaggle/working/modelsFeb20/disc.h5')\ndisc.save_weights('/kaggle/working/modelsFeb20/disc_weight.keras')","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:07.024903Z","iopub.execute_input":"2024-03-02T18:43:07.025236Z","iopub.status.idle":"2024-03-02T18:43:08.789079Z","shell.execute_reply.started":"2024-03-02T18:43:07.025212Z","shell.execute_reply":"2024-03-02T18:43:08.788175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_audio = load('/kaggle/input/music-for-gan/music/val/music_147.wav')\n# Audio(audio.numpy().reshape([1323000]), rate=44100)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:08.790727Z","iopub.execute_input":"2024-03-02T18:43:08.791011Z","iopub.status.idle":"2024-03-02T18:43:08.794878Z","shell.execute_reply.started":"2024-03-02T18:43:08.790986Z","shell.execute_reply":"2024-03-02T18:43:08.794048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"number_of_chunks = mixture_spectogram.shape[0]//256\ntarget = np.zeros([mixture_spectogram.shape[0],512])\nfor i in range(number_of_chunks):\n    START = i*256\n    END = START + 256\n\n    S_mix_new=mixture_spectogram[START:END, :]\n\n    X=tf.reshape(S_mix_new, (1, 256, 512, 1))\n\n    y=gen.predict(X, batch_size=32)\n    target[START:END,:] = y.reshape(256,512)\nS_mix_new=mixture_spectogram[-256:, :]\nX=tf.reshape(S_mix_new,(1, 256, 512, 1))\ny=gen.predict(X, batch_size=32)\ntarget[-256:,:] = y.reshape(256,512)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:38.356006Z","iopub.execute_input":"2024-03-02T18:43:38.356456Z","iopub.status.idle":"2024-03-02T18:43:40.078298Z","shell.execute_reply.started":"2024-03-02T18:43:38.35642Z","shell.execute_reply":"2024-03-02T18:43:40.077477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = tfio.audio.inverse_spectrogram(target.astype(np.float32),nfft=1022, window=1022, stride=256, iterations = 100)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:52.174707Z","iopub.execute_input":"2024-03-02T18:43:52.175359Z","iopub.status.idle":"2024-03-02T18:43:53.783183Z","shell.execute_reply.started":"2024-03-02T18:43:52.175327Z","shell.execute_reply":"2024-03-02T18:43:53.782195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(y,rate=44100)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T18:43:55.72499Z","iopub.execute_input":"2024-03-02T18:43:55.725335Z","iopub.status.idle":"2024-03-02T18:43:55.784099Z","shell.execute_reply.started":"2024-03-02T18:43:55.725307Z","shell.execute_reply":"2024-03-02T18:43:55.782932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}