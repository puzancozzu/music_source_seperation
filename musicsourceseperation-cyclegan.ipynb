{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Music_Generator_CycleGan_single_gen Muse DB.ipynb","provenance":[],"toc_visible":true,"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7590172,"sourceType":"datasetVersion","datasetId":3661086},{"sourceId":7654910,"sourceType":"datasetVersion","datasetId":4300873},{"sourceId":7676046,"sourceType":"datasetVersion","datasetId":4477689},{"sourceId":7698222,"sourceType":"datasetVersion","datasetId":4493109},{"sourceId":7705135,"sourceType":"datasetVersion","datasetId":4498347}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Installing Required Libraries**\n\nthis GAN arch is taken from this git-hub repo : https://github.com/mostafaelaraby/cyclic-gan-music-source-separation","metadata":{}},{"cell_type":"code","source":"!pip install pescador==0.1.3\n!pip install museval","metadata":{"id":"HEP6rjTRGuWC","outputId":"53c53f88-264e-4135-93f1-28d51a923335","execution":{"iopub.status.busy":"2024-03-02T07:09:52.690421Z","iopub.execute_input":"2024-03-02T07:09:52.690810Z","iopub.status.idle":"2024-03-02T07:10:22.295348Z","shell.execute_reply.started":"2024-03-02T07:09:52.690778Z","shell.execute_reply":"2024-03-02T07:10:22.294281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lmdb\n!pip install joblib==1.0.0","metadata":{"id":"h40ZAyQD9sXb","outputId":"7bc558ec-e9a6-44bd-c7c4-08554c5fc4bd","execution":{"iopub.status.busy":"2024-03-02T07:10:22.297483Z","iopub.execute_input":"2024-03-02T07:10:22.297852Z","iopub.status.idle":"2024-03-02T07:10:47.357448Z","shell.execute_reply.started":"2024-03-02T07:10:22.297818Z","shell.execute_reply":"2024-03-02T07:10:47.356310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:10:51.000913Z","iopub.execute_input":"2024-03-02T07:10:51.001284Z","iopub.status.idle":"2024-03-02T07:11:03.450723Z","shell.execute_reply.started":"2024-03-02T07:10:51.001252Z","shell.execute_reply":"2024-03-02T07:11:03.449655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing all the Necessary Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport math \nimport random\nimport librosa\nimport librosa.display\nimport soundfile as sf\nimport lmdb\n\nimport numpy as np\nfrom torch.utils import data\n\nimport glob\nfrom scipy.io.wavfile import read as wavread #TODO needs to check how to make the reader faster\nimport pescador\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F  \nimport torch.optim as optim\nfrom torch.autograd import grad, Variable\nfrom torchsummary import summary  \nimport logging \n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport matplotlib\nimport copy\nfrom IPython.display import HTML\n\nfrom tqdm import tqdm_notebook as tqdm","metadata":{"id":"slOot9JrG1FK","execution":{"iopub.status.busy":"2024-03-02T07:11:03.453034Z","iopub.execute_input":"2024-03-02T07:11:03.453665Z","iopub.status.idle":"2024-03-02T07:11:07.190631Z","shell.execute_reply.started":"2024-03-02T07:11:03.453628Z","shell.execute_reply":"2024-03-02T07:11:07.189713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Config**\n","metadata":{"id":"DyA4N5jdIhh-"}},{"cell_type":"code","source":"#############################\n# Model Params\n#############################\nmodel_prefix = 'exp_musdb_1_wide_unpaired_ralsgan_4'#'exp_large_1' # name of the model to be saved\nn_iterations = 100000   ### 1000000\n# for the cyclic gan use these param with ncritic 1\n# but for training wavegan model it is better to use ttur lr_g = 1e-4 lr_d=3e-4 and n_critic=1\nlr_g = 2e-4\nlr_d = 2e-4\nbeta1 = 0.5\nbeta2 = 0.9\ndecay_lr = False\ngenerator_batch_size_factor = 1 # in some cases we might try to update the generator with double batch size used in the discriminator\nn_critic = 5 # update generator every n_critic steps \n# gradient penalty regularization factor.\np_coeff = 10\nbatch_size = 10\nnoise_latent_dim = 100 \nmodel_capacity_size = 64 # reduce the capacity to 32 for faster training also in case of generating larger window size","metadata":{"id":"yrCdycz2HFBK","execution":{"iopub.status.busy":"2024-03-02T07:17:15.027973Z","iopub.execute_input":"2024-03-02T07:17:15.028336Z","iopub.status.idle":"2024-03-02T07:17:15.034985Z","shell.execute_reply.started":"2024-03-02T07:17:15.028304Z","shell.execute_reply":"2024-03-02T07:17:15.033957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rate of storing validation and costs params\nstore_cost_every = 300\nprogress_bar_step_iter_size = 400","metadata":{"id":"FEeBC1QvQh0h","execution":{"iopub.status.busy":"2024-03-02T07:14:13.679357Z","iopub.execute_input":"2024-03-02T07:14:13.679691Z","iopub.status.idle":"2024-03-02T07:14:13.684042Z","shell.execute_reply.started":"2024-03-02T07:14:13.679664Z","shell.execute_reply":"2024-03-02T07:14:13.683120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# backup params\ntake_backup = True\nbackup_every_n_iters = 10000     #1000\nsave_samples_every = 10000    #1000\n\n# folder including data under each folder train, valid and test subfolders\ntarget_signals_dir = '/kaggle/input/nepali-music-source-seperation/nepali_music_source_seperation/train/vocals' #'/kaggle/input/to-delete-renamed/Newari_song/Newari_song/Bansuri'\nother_signals_dir  = '/kaggle/input/nepali-music-source-seperation/nepali_music_source_seperation/train/mixture'  #'/kaggle/input/to-delete-renamed/Newari_song/Newari_song/mixture'\n\noutput_dir = '/kaggle/working/output'","metadata":{"id":"iZF94hUxIqjO","execution":{"iopub.status.busy":"2024-03-02T07:14:14.125918Z","iopub.execute_input":"2024-03-02T07:14:14.126674Z","iopub.status.idle":"2024-03-02T07:14:14.132050Z","shell.execute_reply.started":"2024-03-02T07:14:14.126641Z","shell.execute_reply":"2024-03-02T07:14:14.130908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#############################\n# Audio Reading Params\n#############################\nwindow_length = 16384 #[16384, 32768, 65536] change model_capacity size to 32 in case of slice>16384\nsampling_rate = 16000\nnormalize_audio = True ","metadata":{"id":"qO0VUGnJI_mC","execution":{"iopub.status.busy":"2024-03-02T07:14:14.891787Z","iopub.execute_input":"2024-03-02T07:14:14.892680Z","iopub.status.idle":"2024-03-02T07:14:14.897275Z","shell.execute_reply.started":"2024-03-02T07:14:14.892646Z","shell.execute_reply":"2024-03-02T07:14:14.896230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#############################\n# Torch Init and seed setting\n#############################\ncuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n# update the seed\nmanual_seed = 2014 #@param {type: \"integer\"}\n#manualSeed = random.randint(1, 10000) # use if you want new results\nprint(\"Random Seed: \", manual_seed)\nrandom.seed(manual_seed)\ntorch.manual_seed(manual_seed)\nnp.random.seed(manual_seed)\nif cuda:\n    torch.cuda.manual_seed(manual_seed)\n    torch.cuda.empty_cache()","metadata":{"id":"tI5ZYc_1JCxH","outputId":"579bf630-0c5b-4d78-c8a4-cc25c0cfc5f0","execution":{"iopub.status.busy":"2024-03-02T07:14:17.305637Z","iopub.execute_input":"2024-03-02T07:14:17.306467Z","iopub.status.idle":"2024-03-02T07:14:17.314619Z","shell.execute_reply.started":"2024-03-02T07:14:17.306435Z","shell.execute_reply":"2024-03-02T07:14:17.313512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#############################\n# Logger init\n#############################\nLOGGER = logging.getLogger('/kaggle/working/wavegan')\nLOGGER.setLevel(logging.DEBUG)","metadata":{"id":"QPKhpO3rJCOr","execution":{"iopub.status.busy":"2024-03-02T07:14:17.848422Z","iopub.execute_input":"2024-03-02T07:14:17.849145Z","iopub.status.idle":"2024-03-02T07:14:17.853628Z","shell.execute_reply.started":"2024-03-02T07:14:17.849107Z","shell.execute_reply":"2024-03-02T07:14:17.852622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{"id":"BLMhCmpbHeJm"}},{"cell_type":"markdown","source":"### File Utils","metadata":{"id":"s9HuhlS9ozMA"}},{"cell_type":"code","source":"def get_recursive_files(folderPath,ext):\n\t\tresults  = os.listdir(folderPath)\n\t\toutFiles = [] \n\t\tfor file in results:\n\t\t\tif os.path.isdir(os.path.join(folderPath,file)):\n\t\t\t\toutFiles+=get_recursive_files(os.path.join(folderPath,file),ext)\n\t\t\telif file.endswith(ext) :\n\t\t\t\toutFiles.append(os.path.join(folderPath,file))\n\t\t\t\n\t\treturn outFiles\n\ndef make_path(output_path):\n    if not os.path.isdir(output_path):\n        os.makedirs(output_path)\n    return output_path","metadata":{"id":"vm2BniJ6HQsS","execution":{"iopub.status.busy":"2024-03-02T07:14:20.841473Z","iopub.execute_input":"2024-03-02T07:14:20.841824Z","iopub.status.idle":"2024-03-02T07:14:20.848559Z","shell.execute_reply.started":"2024-03-02T07:14:20.841795Z","shell.execute_reply":"2024-03-02T07:14:20.847577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Wav files utils","metadata":{"id":"K1tRItDco3bK"}},{"cell_type":"code","source":"#Fast loading used with wav files only of 8 bits\ndef load_wav(wav_file_path, fast_loading=False ):\n    try:\n        if fast_loading:\n            file_sampling_rate, audio_data = wavread(wav_file_path)\n            if file_sampling_rate is not None and sampling_rate != file_sampling_rate:\n                raise NotImplementedError('Scipy cannot resample audio.')\n            if audio_data.dtype == np.int16:\n                audio_data = audio_data.astype(np.float32)\n                audio_data /= 32768.\n            elif audio_data.dtype == np.float32:\n                audio_data = np.copy(audio_data)\n            else:\n                raise NotImplementedError('Scipy cannot process atypical WAV files.')\n            \n        else:\n            audio_data, _ = librosa.load(wav_file_path, sr=sampling_rate)\n\n        if normalize_audio:\n            # Clip magnitude\n            max_mag = np.max(np.abs(audio_data))\n            if max_mag > 1:\n                audio_data /= max_mag\n    except Exception as e:\n        print(wav_file_path)\n        print(str(e))\n        LOGGER.error(\"Could not load {}: {}\".format(wav_file_path, str(e)))\n        raise e\n    audio_len = len(audio_data)\n    if audio_len < window_length:\n        pad_length = window_length - audio_len\n        left_pad = pad_length // 2\n        right_pad = pad_length - left_pad \n        audio_data = np.pad(audio_data, (left_pad, right_pad), mode='constant')\n        \n    \n    return audio_data.astype('float32')\n\n\ndef save_samples(epoch_samples, epoch ,prefix=''):\n    \"\"\"\n    Save output samples.\n    \"\"\"\n    sample_dir = make_path(os.path.join(output_dir, str(epoch)))\n\n    for idx, sample in enumerate(epoch_samples):\n        output_path = os.path.join(sample_dir, \"{}_{}.wav\".format(prefix,idx+1))\n        sample = sample[0]\n        sf.write(output_path, sample, sampling_rate)","metadata":{"id":"RJG0ti9RG1sp","execution":{"iopub.status.busy":"2024-03-02T07:14:21.802476Z","iopub.execute_input":"2024-03-02T07:14:21.802825Z","iopub.status.idle":"2024-03-02T07:14:21.814272Z","shell.execute_reply.started":"2024-03-02T07:14:21.802797Z","shell.execute_reply":"2024-03-02T07:14:21.813346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_audio(audio_data, start_idx=None, end_idx=None):\n    audio_len = len(audio_data)\n    if audio_len == window_length:\n        # If we only have a single 1*window_length audio, just yield.\n        sample = audio_data\n    else:\n        # Sample a random window from the audio\n        if start_idx is None or end_idx is None:\n            start_idx = np.random.randint(0, (audio_len - window_length) // 2)\n            end_idx = start_idx + window_length\n        sample = audio_data[start_idx:end_idx]\n    sample =  sample.astype('float32')\n    assert not np.any(np.isnan(sample))\n    return sample, start_idx, end_idx\n\ndef audio_generator(audio_data):\n    audio_len = len(audio_data)\n    n_iters = audio_len // window_length\n    for i in range(n_iters+1):\n        start_idx = i * window_length\n        end_idx = start_idx  + window_length\n        result = np.zeros(window_length)\n        audio_size = audio_data[start_idx:end_idx].shape[0]\n        result[:audio_size] = audio_data[start_idx:end_idx]\n        yield result\n    \n    \ndef sample_buffer(buffer_data, start_idx=None, end_idx=None):\n    audio_len = len(buffer_data) // 4\n    if audio_len == window_length:\n        # If we only have a single 1*window_length audio, just yield.\n        sample = buffer_data\n    else:\n        # Sample a random window from the audio\n        if start_idx is None or end_idx is None:\n            start_idx = np.random.randint(0, (audio_len - window_length) // 2)\n            end_idx = start_idx + window_length\n        sample = buffer_data[start_idx * 4:end_idx*4]\n    return sample, start_idx, end_idx\n\n\ndef wav_generator(file_path, mixing_signal_path):\n    audio_data = load_wav(file_path)\n    mixing_data = load_wav(mixing_signal_path)\n    while True:\n        sample, start_idx, end_idx = sample_audio(audio_data)\n        mixing_sample, _, _ = sample_audio(mixing_data,start_idx, end_idx)\n\n        mixing_ratio = np.random.uniform(0, 1)\n        mixed_signal = mixing_ratio * mixing_sample + (1-mixing_ratio) * sample\n\n        yield {'single':sample, 'mixed':mixed_signal}\n\ndef create_stream_reader(single_signal_file_list, other_signal_file_list):\n    data_streams = []\n    other_signal_len  = len(other_signal_file_list)\n    for audio_path in single_signal_file_list:\n        other_signal_indx = np.random.randint(0,other_signal_len)\n        stream = pescador.Streamer(wav_generator, audio_path, other_signal_file_list[other_signal_indx])\n        data_streams.append(stream)\n    mux = pescador.ShuffledMux(data_streams)\n    batch_gen = pescador.buffer_stream(mux, batch_size)\n    return batch_gen","metadata":{"id":"offZXw-rHWzw","execution":{"iopub.status.busy":"2024-03-02T07:14:22.305352Z","iopub.execute_input":"2024-03-02T07:14:22.305975Z","iopub.status.idle":"2024-03-02T07:14:22.319980Z","shell.execute_reply.started":"2024-03-02T07:14:22.305922Z","shell.execute_reply":"2024-03-02T07:14:22.319068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Utils","metadata":{"id":"NKeUFiF0o-Jh"}},{"cell_type":"code","source":"\ndef sample_noise(size):\n    z = torch.FloatTensor(size,noise_latent_dim).to(device)\n    z.data.normal_() # generating latent space based on normal distribution\n    return z\n\ndef weights_init(m): \n    if isinstance(m, nn.Conv1d):\n        m.weight.data.normal_(0.0, 0.02)\n        if m.bias is not None:\n            m.bias.data.fill_(0)\n        m.bias.data.fill_(0)\n    elif isinstance(m, nn.Linear):\n        m.bias.data.fill_(0)\n\ndef update_optimizer_lr(optimizer,lr,decay):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr* decay   \n\n\ndef gradients_status(model,flag):\n    for p in model.parameters():\n        p.requires_grad = flag","metadata":{"id":"5-YmpyOZHNk8","execution":{"iopub.status.busy":"2024-03-02T07:14:23.280951Z","iopub.execute_input":"2024-03-02T07:14:23.281851Z","iopub.status.idle":"2024-03-02T07:14:23.289231Z","shell.execute_reply.started":"2024-03-02T07:14:23.281818Z","shell.execute_reply":"2024-03-02T07:14:23.288259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# used to replay some history generate to the discriminator to avoid greedy disc.\nclass ReplayBuffer():\n    def __init__(self, max_size=50):\n        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0,1) > 0.5:\n                    i = random.randint(0, self.max_size-1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return Variable(torch.cat(to_return))","metadata":{"id":"1Qcmv23crtN_","execution":{"iopub.status.busy":"2024-03-02T07:14:24.156366Z","iopub.execute_input":"2024-03-02T07:14:24.156745Z","iopub.status.idle":"2024-03-02T07:14:24.165344Z","shell.execute_reply.started":"2024-03-02T07:14:24.156716Z","shell.execute_reply":"2024-03-02T07:14:24.164332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loader and Sampler","metadata":{"id":"lgCxXMPApNo-"}},{"cell_type":"code","source":"def numpy_to_tensor(numpy_array):\n    numpy_array = numpy_array[:, np.newaxis, :]\n    return torch.Tensor(numpy_array).to(device)\n\n#############################\n# Creating Data Loader and Sampler\n#############################\nclass WavDataLoader():\n    def __init__(self, folder_path, other_signals_folder, audio_extension='wav'):\n        self.signal_paths = get_recursive_files(folder_path, audio_extension)\n        self.mixed_wav_files = get_recursive_files(other_signals_folder, audio_extension)\n        self.data_iter = None\n        self.initialize_iterator()\n\n    def initialize_iterator(self):\n        data_iter = create_stream_reader(self.signal_paths, self.mixed_wav_files)\n        self.data_iter = iter(data_iter)\n\n    def __len__(self):\n        return len(self.signal_paths)\n        \n\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        x =  next(self.data_iter)\n        return (numpy_to_tensor(x['single']), numpy_to_tensor(x['mixed']), numpy_to_tensor(x['foreground']))\n\n","metadata":{"id":"QKOlZPMOHK6N","execution":{"iopub.status.busy":"2024-03-02T07:14:25.115217Z","iopub.execute_input":"2024-03-02T07:14:25.115918Z","iopub.status.idle":"2024-03-02T07:14:25.123914Z","shell.execute_reply.started":"2024-03-02T07:14:25.115889Z","shell.execute_reply":"2024-03-02T07:14:25.123001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LMDBWavLoader(data.Dataset):\n    def __init__(self, lmdb_file_path, is_test=False):\n        self.env = lmdb.open(lmdb_file_path, max_readers=1, readonly=True, lock=False,\n                             readahead=False, meminit=False)\n        self.datum = datanum_pb2.DataNum()\n        self.is_test = is_test\n        \n\n    def __len__(self):\n        n_entries =  int(self.env.stat()['entries'])\n        return n_entries\n\n\n    def __getitem__(self, data_indx):\n        index = None\n        audio_indx = None\n        index = data_indx\n        with self.env.begin(write=False) as cursor:\n            raw_datum = cursor.get('{:08}'.format(index).encode('ascii'))\n        self.datum.ParseFromString(raw_datum)\n        # float is represented by 4 bytes\n        start_idx = None\n        end_idx = None\n        if self.is_test:\n            return np.array(np.frombuffer(self.datum.vocals, dtype=np.float32)).reshape(-1),np.array(np.frombuffer(self.datum.mixture, dtype=np.float32)).reshape(-1)\n                \n        mixture, start_idx, end_idx = sample_buffer(self.datum.mixture, start_idx, end_idx)\n        mixture = np.array(np.frombuffer(mixture, dtype=np.float32)).reshape(1,-1)\n        vocals, _, _ = sample_buffer(self.datum.vocals, start_idx, end_idx)\n        vocals = np.frombuffer(vocals, dtype=np.float32).reshape(1,-1)\n        return vocals, mixture","metadata":{"id":"uN8qDopu9sYm","execution":{"iopub.status.busy":"2024-03-02T07:14:25.569053Z","iopub.execute_input":"2024-03-02T07:14:25.569636Z","iopub.status.idle":"2024-03-02T07:14:25.579944Z","shell.execute_reply.started":"2024-03-02T07:14:25.569609Z","shell.execute_reply":"2024-03-02T07:14:25.578985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize utils","metadata":{"id":"vtrJ-eP0pQjS"}},{"cell_type":"code","source":"def visualize_audio(audio_tensor, is_monphonic=False):\n    # takes a batch ,n channels , window length and plots the spectogram\n    input_audios = audio_tensor.detach().cpu().numpy()\n    plt.figure(figsize=(18, 50))\n    for i , audio in enumerate(input_audios):\n        plt.subplot(10, 2, i+1)\n        if is_monphonic:\n            plt.title('Monophonic %i' % (i+1))\n            librosa.display.waveplot(audio[0], sr=sampling_rate)\n        else:\n            D = librosa.amplitude_to_db(np.abs(librosa.stft(audio[0])), ref=np.max)\n            librosa.display.specshow(D, y_axis='linear')\n            plt.colorbar(format='%+2.0f dB')\n            plt.title('Linear-frequency power spectrogram %i' % (i+1))\n    plt.show()\n\ndef visualize_loss(loss_1, loss_2, first_legend, second_legend, y_label):\n    plt.figure(figsize=(10,5))\n    plt.title(\"{} and {} Loss During Training\".format(first_legend, second_legend))\n    plt.plot(loss_1,label=first_legend)\n    plt.plot(loss_2,label=second_legend)\n    plt.xlabel(\"iterations\")\n    plt.ylabel(y_label)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.legend()\n    plt.show()","metadata":{"id":"ukzdK3k6MTnP","execution":{"iopub.status.busy":"2024-03-02T07:14:26.526730Z","iopub.execute_input":"2024-03-02T07:14:26.527101Z","iopub.status.idle":"2024-03-02T07:14:26.536806Z","shell.execute_reply.started":"2024-03-02T07:14:26.527074Z","shell.execute_reply":"2024-03-02T07:14:26.536010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def latent_space_interpolation(model, n_samples=10 ):\n    z_test = sample_noise(2)\n    with torch.no_grad():  \n        interpolates = []\n        for alpha in np.linspace(0, 1, n_samples):\n            interpolate_vec = alpha * z_test[0] + ((1 - alpha) * z_test[1])\n            interpolates.append(interpolate_vec)     \n            \n        interpolates = torch.stack(interpolates)\n        generated_audio = model(interpolates)\n    visualize_audio(generated_audio, True)","metadata":{"id":"PxHweiv2pW-5","execution":{"iopub.status.busy":"2024-03-02T07:14:27.053073Z","iopub.execute_input":"2024-03-02T07:14:27.053433Z","iopub.status.idle":"2024-03-02T07:14:27.059435Z","shell.execute_reply.started":"2024-03-02T07:14:27.053406Z","shell.execute_reply":"2024-03-02T07:14:27.058553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isdir(output_dir):\n  os.system('rm -r {}'.format(output_dir))\nmake_path(output_dir)\n","metadata":{"id":"7GGWmjTDIyIv","outputId":"3e23f00e-b246-4f35-d9f9-46a27158950f","execution":{"iopub.status.busy":"2024-03-02T07:14:27.445016Z","iopub.execute_input":"2024-03-02T07:14:27.445641Z","iopub.status.idle":"2024-03-02T07:14:27.456389Z","shell.execute_reply.started":"2024-03-02T07:14:27.445613Z","shell.execute_reply":"2024-03-02T07:14:27.455510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Loading**","metadata":{"id":"3PgUiN6uJYtz"}},{"cell_type":"code","source":"!git clone https://github.com/mostafaelaraby/cyclic-gan-music-source-separation.git\n    \n### this code in some dependecy file from this git hub repo","metadata":{"id":"RCG_OJ0wJxr2","execution":{"iopub.status.busy":"2024-03-02T07:14:29.601615Z","iopub.execute_input":"2024-03-02T07:14:29.601986Z","iopub.status.idle":"2024-03-02T07:14:30.545938Z","shell.execute_reply.started":"2024-03-02T07:14:29.601953Z","shell.execute_reply":"2024-03-02T07:14:30.544985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/cyclic-gan-music-source-separation')","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:14:30.547899Z","iopub.execute_input":"2024-03-02T07:14:30.548217Z","iopub.status.idle":"2024-03-02T07:14:30.552824Z","shell.execute_reply.started":"2024-03-02T07:14:30.548189Z","shell.execute_reply":"2024-03-02T07:14:30.551859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from protocol_buffer import datanum_pb2","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:14:31.449350Z","iopub.execute_input":"2024-03-02T07:14:31.449707Z","iopub.status.idle":"2024-03-02T07:14:31.454441Z","shell.execute_reply.started":"2024-03-02T07:14:31.449678Z","shell.execute_reply":"2024-03-02T07:14:31.453487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport lmdb\n## import caffe\n## from   utils  import get_recursive_files, sample_audio, load_wav, manual_seed\nmanual_seed=2014 \nsampling_rate=16000 \nwindow_length=16384\nfrom sklearn.model_selection import train_test_split\nimport wave\nimport librosa\nfrom tqdm import tqdm\n \n## protoc -I=$SRC_DIR --python_out=$DST_DIR/datanum_pb2.py $SRC_DIR/datanum.proto\n## to create protoc\n\n\ndef get_map_size(files):\n    return load_wav(files[0]).nbytes * 10 * (len(files) +2)\n\ndef get_silent_set(input_audio):\n    indices = np.where(input_audio==0)[0]\n    index_sets = []\n    window = 16384\n    counter = 0\n    prev_index = -1\n    first_index = -1\n    for index in indices:\n        if counter==0:\n            first_index = index\n        if index - prev_index == 1:\n            counter +=1\n        else:\n            if counter>window:\n                index_sets.append((first_index, prev_index, counter))\n            counter = 0\n        prev_index = index\n    if counter>window:\n        index_sets.append((first_index, prev_index, counter))\n    return index_sets\n\ndef remove_silence(input_audio, index_sets):\n    for silent_indices in index_sets:\n        first_indx = silent_indices[0]\n        last_idx = silent_indices[1]\n        input_audio = np.delete(input_audio, [indx for indx in range(first_indx, last_idx)])\n    return input_audio\n    \n## adapted from \n## https://github.com/francesclluis/source-separation-wavenet/blob/6d89618c77d38960c3996219f329e5806573799b/util.py#L220\n## return start and ending indices first one is the start second on is the finish\n\ndef get_sequence_with_singing_indices(full_sequence, chunk_length = 800):\n\n    signal_magnitude = np.abs(full_sequence)\n\n    chunks_energies = []\n    for i in range(0, len(signal_magnitude), chunk_length):\n        chunks_energies.append(np.mean(signal_magnitude[i:i + chunk_length]))\n\n    threshold = np.max(chunks_energies) * .1\n    chunks_energies = np.asarray(chunks_energies)\n    chunks_energies[np.where(chunks_energies < threshold)] = 0\n    onsets = np.zeros(len(chunks_energies))\n    onsets[np.nonzero(chunks_energies)] = 1\n    onsets = np.diff(onsets)\n\n    start_ind = np.squeeze(np.where(onsets == 1))\n    finish_ind = np.squeeze(np.where(onsets == -1))\n\n    if finish_ind[0] < start_ind[0]:\n        finish_ind = finish_ind[1:]\n\n    if start_ind[-1] > finish_ind[-1]:\n        start_ind = start_ind[:-1]\n\n    indices_inici_final = np.insert(finish_ind, np.arange(len(start_ind)), start_ind)\n\n    return np.squeeze((np.asarray(indices_inici_final) + 1) * chunk_length)\n\ndef write_lmdb(out_file_name, data_list):\n    lmdb_output = lmdb.open(out_file_name, map_size=get_map_size(data_list))\n    with lmdb_output.begin(write=True) as txn:\n        # txn is a Transaction object\n        for audio_indx, audio_path in enumerate(tqdm(data_list)):\n            mixed_data = load_wav(audio_path).astype('float32')\n            vocals_data = load_wav(audio_path.replace('mixture','vocals')).astype('float32')   #Bansuri---->vocals\n            '''\n            # to remove zeros from mixed and vocals based on vocals\n            silent_set = get_silent_set(vocals_data)\n            mixed_data = remove_silence(mixed_data, silent_set)\n            vocals_data = remove_silence(vocals_data, silent_set)\n            '''\n\n            vocals_indices = get_sequence_with_singing_indices(vocals_data, 800)\n            \n\n            datum = datanum_pb2.DataNum()\n            datum.mixture = mixed_data.tobytes()\n            datum.vocals = vocals_data.tobytes() \n            datum.vocals_indices = vocals_indices.tobytes() # used to store the indices having voice\n            str_id = '{:08}'.format(audio_indx)\n            txn.put(str_id.encode('ascii'), datum.SerializeToString()) \n\ndef create_lmdb(folder_name, out_file_name, is_train=True):\n    mixture_audio_train = get_recursive_files(folder_name,'.wav')\n    #     print(audio_train)\n    audio_valid = True\n    if is_train:\n        # create validation set\n        mixture_audio_train, mixture_audio_valid = train_test_split(mixture_audio_train, test_size = 0.15, random_state = manual_seed)\n    write_lmdb(out_file_name, mixture_audio_train)\n\n    if audio_valid:\n        write_lmdb(out_file_name.replace('_train','')+'_valid', mixture_audio_valid)\n\nparent_folder = \"/kaggle/input/nepali-music-source-seperation/nepali_music_source_seperation/train/mixture\" ##\"/kaggle/input/newari-music/Newari_song/mixture\"\ncreate_lmdb(parent_folder, 'musdb_train')\n\n### parent_folder = \"/kaggle/input/musdb18ying/test\"\n### create_lmdb(parent_folder, 'musdb_test', False)\n\n\n### Testing Proto buffer reading\nfor lmdb_name in ['musdb_train', 'musdb_valid']:\n    env = lmdb.open(lmdb_name, readonly=True)\n    with env.begin() as txn:\n        raw_datum = txn.get(b'00000000')\n\n    datum = datanum_pb2.DataNum()\n    datum.ParseFromString(raw_datum)\n\n    mixture = np.fromstring(datum.mixture, dtype=np.float32)\n    vocals = np.fromstring(datum.vocals, dtype=np.float32)\n    vocals_indices = np.fromstring(datum.vocals_indices, dtype=np.int32)\n    print(mixture.shape)\n    print(vocals.shape)\n    print(vocals_indices)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:15:29.324057Z","iopub.execute_input":"2024-03-02T07:15:29.324445Z","iopub.status.idle":"2024-03-02T07:15:48.517628Z","shell.execute_reply.started":"2024-03-02T07:15:29.324399Z","shell.execute_reply":"2024-03-02T07:15:48.516519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading data using data loader\ntrain_dataset = LMDBWavLoader('/kaggle/working/musdb_train')  ##'/kaggle/working/musdb_train'\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n                                        shuffle=True, num_workers=1,drop_last=True,pin_memory=True)\nval_dataset = LMDBWavLoader('/kaggle/working/musdb_valid')\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n                                        shuffle=True, num_workers=1,drop_last=True,pin_memory=True)\nprint(val_dataset,train_dataset,train_loader,val_loader)","metadata":{"id":"oEO_FDuOKfId","execution":{"iopub.status.busy":"2024-03-02T07:16:05.711100Z","iopub.execute_input":"2024-03-02T07:16:05.711817Z","iopub.status.idle":"2024-03-02T07:16:05.719330Z","shell.execute_reply.started":"2024-03-02T07:16:05.711787Z","shell.execute_reply":"2024-03-02T07:16:05.718439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"id":"mvsDhwKeHf4r"}},{"cell_type":"code","source":"class Transpose1dLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=11, upsample=None, output_padding=1, use_batch_norm = False):\n        super(Transpose1dLayer, self).__init__()\n        self.upsample = upsample\n        reflection_pad = nn.ConstantPad1d(kernel_size // 2, value=0)\n        conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride)\n        conv1d.weight.data.normal_(0.0, 0.02)\n        Conv1dTrans = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride, padding, output_padding)\n        batch_norm = nn.BatchNorm1d(out_channels)\n        if self.upsample:\n            operation_list = [\n                reflection_pad,\n                conv1d  \n            ]\n        else:\n            operation_list = [\n                Conv1dTrans\n            ]\n\n        if use_batch_norm:\n            operation_list.append(batch_norm)\n        self.transpose_ops = nn.Sequential(*operation_list )\n\n    def forward(self, x):\n        if self.upsample:\n            # recommended by wavgan paper to use nearest upsampling\n            x = nn.functional.interpolate(x,scale_factor=self.upsample, mode='nearest')\n        return self.transpose_ops(x)\n","metadata":{"id":"uu2hKN9kHgsl","execution":{"iopub.status.busy":"2024-03-02T07:16:09.116209Z","iopub.execute_input":"2024-03-02T07:16:09.117152Z","iopub.status.idle":"2024-03-02T07:16:09.125797Z","shell.execute_reply.started":"2024-03-02T07:16:09.117122Z","shell.execute_reply":"2024-03-02T07:16:09.124841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv1D(nn.Module):\n    def __init__(self,input_channels, output_channels, kernel_size,alpha = 0.2,shift_factor=2, stride=4, padding=11, use_batch_norm=False, drop_prob = 0):\n        super(Conv1D, self).__init__()\n        self.conv1d = nn.Conv1d(input_channels, output_channels, kernel_size, stride=stride, padding=padding)\n        self.batch_norm = nn.BatchNorm1d(output_channels)\n        self.phase_shuffle =  PhaseShuffle(shift_factor)\n        self.alpha = alpha\n        self.use_batch_norm = use_batch_norm\n        self.use_phase_shuffle = shift_factor==0\n        self.use_drop = drop_prob>0\n        self.dropout = nn.Dropout2d(drop_prob)\n    \n    def forward(self, x):\n        x = self.conv1d(x)\n        if self.use_batch_norm:\n            x = self.batch_norm(x)\n        x = F.leaky_relu(x, negative_slope=self.alpha)\n        if self.use_phase_shuffle: \n            x = self.phase_shuffle(x)\n        if self.use_drop:\n            x = self.dropout(x)\n        return x","metadata":{"id":"o5U4rCRHHiJp","execution":{"iopub.status.busy":"2024-03-02T07:16:11.635746Z","iopub.execute_input":"2024-03-02T07:16:11.636435Z","iopub.status.idle":"2024-03-02T07:16:11.645107Z","shell.execute_reply.started":"2024-03-02T07:16:11.636404Z","shell.execute_reply":"2024-03-02T07:16:11.644104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PhaseShuffle(nn.Module):\n    \"\"\"\n    Performs phase shuffling, i.e. shifting feature axis of a 3D tensor\n    by a random integer in {-n, n} and performing reflection padding where\n    necessary.\n    \"\"\"\n    # Copied from https://github.com/jtcramer/wavegan/blob/master/wavegan.py#L8\n    def __init__(self, shift_factor):\n        super(PhaseShuffle, self).__init__()\n        self.shift_factor = shift_factor\n\n    def forward(self, x):\n        if self.shift_factor == 0:\n            return x\n        # uniform in (L, R)\n        k_list = torch.Tensor(x.shape[0]).random_(0, 2 * self.shift_factor + 1) - self.shift_factor\n        k_list = k_list.numpy().astype(int)\n\n        # Combine sample indices into lists so that less shuffle operations\n        # need to be performed\n        k_map = {}\n        for idx, k in enumerate(k_list):\n            k = int(k)\n            if k not in k_map:\n                k_map[k] = []\n            k_map[k].append(idx)\n\n        # Make a copy of x for our output\n        x_shuffle = x.clone()\n\n        # Apply shuffle to each sample\n        for k, idxs in k_map.items():\n            if k > 0:\n                x_shuffle[idxs] = F.pad(x[idxs][..., :-k], (k, 0), mode='reflect')\n            else:\n                x_shuffle[idxs] = F.pad(x[idxs][..., -k:], (0, -k), mode='reflect')\n\n        assert x_shuffle.shape == x.shape, \"{}, {}\".format(x_shuffle.shape,\n                                                       x.shape)\n        return x_shuffle","metadata":{"id":"tCZoaeDOHjaP","execution":{"iopub.status.busy":"2024-03-02T07:16:13.998620Z","iopub.execute_input":"2024-03-02T07:16:13.999242Z","iopub.status.idle":"2024-03-02T07:16:14.008947Z","shell.execute_reply.started":"2024-03-02T07:16:13.999212Z","shell.execute_reply":"2024-03-02T07:16:14.007950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generator Model","metadata":{"id":"UI5KHANTHnq7"}},{"cell_type":"code","source":"class WaveGANGenerator(nn.Module):\n    def __init__(self, model_size=64, num_channels=1,\n                  verbose=False,\n                  upsample=True, slice_len=16384, use_batch_norm = False):\n        super(WaveGANGenerator, self).__init__()\n        assert slice_len in [16384, 32768, 65536] # used to predict longer utterances\n\n        self.model_size = model_size  # d \n        self.verbose = verbose\n        self.use_batch_norm = use_batch_norm\n\n        self.dim_mul = 16 if slice_len == 16384 else 32\n\n        self.fc1 = nn.Linear(noise_latent_dim, 4*4* model_size * self.dim_mul)\n        self.bn1 = nn.BatchNorm1d(num_features= model_size * self.dim_mul)\n\n        stride = 4\n        if upsample:\n            stride = 1\n            upsample = 4\n        \n        deconv_layers = [\n            Transpose1dLayer( self.dim_mul* model_size, (self.dim_mul* model_size) //2, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm),\n            Transpose1dLayer((self.dim_mul* model_size) //2, (self.dim_mul* model_size) //4, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm),\n            Transpose1dLayer( (self.dim_mul* model_size) //4,  (self.dim_mul* model_size) //8, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm),\n            Transpose1dLayer( (self.dim_mul* model_size) //8,  (self.dim_mul* model_size) //16, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm),\n        ]\n        \n\n        if slice_len== 16384:\n            deconv_layers.append( Transpose1dLayer((self.dim_mul* model_size) //16, num_channels, 25, stride, upsample=upsample))\n        elif slice_len == 32768 :\n            deconv_layers +=[ \n                Transpose1dLayer((self.dim_mul* model_size) //16, model_size, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm)\n                ,Transpose1dLayer(model_size, num_channels, 25, 2, upsample=upsample)\n            ]\n        elif slice_len == 65536:\n            deconv_layers +=[\n                Transpose1dLayer((self.dim_mul* model_size) //16, model_size, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm)\n                ,Transpose1dLayer(model_size, num_channels, 25, stride, upsample=upsample)\n            ]\n        else:\n            raise ValueError('slice_len {} value is not supported'.format(slice_len))\n        \n\n        self.deconv_list = nn.ModuleList(deconv_layers)\n        for m in self.modules():\n            if isinstance(m, nn.ConvTranspose1d) or isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight.data)\n\n    def forward(self, x):\n        x = self.fc1(x).view(-1, self.dim_mul* self.model_size, 16)\n        if self.use_batch_norm:\n            x = self.bn1(x)\n        x = F.relu(x)\n        if self.verbose:\n            print(x.shape)\n\n        for deconv in self.deconv_list[:-1]:\n            x = F.relu(deconv(x))\n            if self.verbose:\n                print(x.shape)\n        output = torch.tanh(self.deconv_list[-1](x))\n        return output","metadata":{"id":"b_pE1roiHlMJ","execution":{"iopub.status.busy":"2024-03-02T07:16:14.501133Z","iopub.execute_input":"2024-03-02T07:16:14.501756Z","iopub.status.idle":"2024-03-02T07:16:14.518391Z","shell.execute_reply.started":"2024-03-02T07:16:14.501726Z","shell.execute_reply":"2024-03-02T07:16:14.517403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discriminator Model","metadata":{"id":"VeGi98DxHqH1"}},{"cell_type":"code","source":"class WaveGANDiscriminator(nn.Module):\n    def __init__(self, model_size=64, ngpus=1, num_channels=1, shift_factor=2,\n                 alpha=0.2, verbose=False, slice_len=16384, use_batch_norm = False):\n        super(WaveGANDiscriminator, self).__init__()\n        assert slice_len in [16384, 32768, 65536] # used to predict longer utterances\n        \n        self.model_size = model_size  # d\n        self.ngpus = ngpus\n        self.use_batch_norm = use_batch_norm\n        self.num_channels = num_channels  # c\n        self.shift_factor = shift_factor  # n\n        self.alpha = alpha\n        self.verbose = verbose\n\n\n        conv_layers = [\n            Conv1D(num_channels, model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            Conv1D(model_size, 2 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            Conv1D(2 * model_size, 4 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            Conv1D(4 * model_size, 8 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            Conv1D(8 * model_size, 16 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=0 if slice_len==16384 else shift_factor)\n        ]\n        self.fc_input_size = 256 * model_size\n        if slice_len == 32768 :\n            conv_layers.append(\n                 Conv1D(16 * model_size, 32 * model_size, 25, stride=2, padding=11,use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=0)\n            ) \n            self.fc_input_size = 480 * model_size\n        elif slice_len == 65536:\n            conv_layers.append(\n                 Conv1D(16 * model_size, 32 * model_size, 25, stride=4, padding=11,use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=0)\n            )\n            self.fc_input_size = 512 * model_size\n        \n        self.conv_layers = nn.ModuleList(conv_layers)\n        \n        self.fc1 = nn.Linear(self.fc_input_size, 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight.data)\n\n    def forward(self, x):\n        for conv in self.conv_layers:\n            x= conv(x)\n            if self.verbose:\n                print(x.shape)\n        x = x.view(-1, self.fc_input_size)\n        if self.verbose:\n            print(x.shape)\n\n        return self.fc1(x)","metadata":{"id":"WL4yeoYXHsNS","execution":{"iopub.status.busy":"2024-03-02T07:16:14.939654Z","iopub.execute_input":"2024-03-02T07:16:14.940343Z","iopub.status.idle":"2024-03-02T07:16:14.955062Z","shell.execute_reply.started":"2024-03-02T07:16:14.940312Z","shell.execute_reply":"2024-03-02T07:16:14.954074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cyclic Generator with resblocks","metadata":{"id":"naAboFqLq9-m"}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_features, use_batch_norm=True, alpha=0.2, shift_factor=2):\n        super(ResidualBlock, self).__init__()\n        conv_blocks = [\n            Conv1D(in_features, in_features, 21, stride=1, padding=10, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            Conv1D(in_features, in_features, 21, stride=1, padding=10, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n        ]\n        self.conv_blocks = nn.ModuleList(conv_blocks)\n    \n    def forward(self, x):\n        #down_sampled_x = nn.functional.interpolate(x,scale_factor=0.25, mode='nearest')\n        output = x\n        for conv in self.conv_blocks:\n            output = conv(output)\n        return x + output","metadata":{"id":"cBkTYn7ErBHB","execution":{"iopub.status.busy":"2024-03-02T07:16:15.350496Z","iopub.execute_input":"2024-03-02T07:16:15.351412Z","iopub.status.idle":"2024-03-02T07:16:15.359237Z","shell.execute_reply.started":"2024-03-02T07:16:15.351379Z","shell.execute_reply":"2024-03-02T07:16:15.358124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CyclicWaveGanGenerator(nn.Module):\n    def __init__(self,model_size=64,  num_channels=1, shift_factor=2,\n                 alpha=0.2, verbose=False, slice_len=16384, use_batch_norm = False, upsample=True):\n        super(CyclicWaveGanGenerator, self).__init__()\n        assert slice_len in [16384, 32768, 65536] # used to predict longer utterances\n        self.dim_mul = 16 if slice_len == 16384 else 32\n        self.verbose = verbose\n        encoder_conv = [ \n            Conv1D(num_channels, model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            Conv1D(model_size, 2 * model_size, 25, stride=4, padding=13 if slice_len==32768 else 11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            #Conv1D(2 * model_size, 4*model_size , 25, stride=4, padding=13 if slice_len==32768 else 11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n        ]\n        n_resblocks = 8\n        if slice_len == 32768:\n            encoder_conv.append(\n                Conv1D(2*model_size , (self.dim_mul* model_size) //8, 25, stride=2, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n            )\n            n_resblocks = 9\n        elif slice_len == 65536:\n            encoder_conv.append(\n                Conv1D(2*model_size , (self.dim_mul* model_size) //8, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n            )\n            n_resblocks = 9\n        \n        self.encoder = nn.Sequential(*encoder_conv)\n        # encoder output 1 256 256\n        \n        transformation = []\n        for _ in range(n_resblocks):\n            transformation.append(ResidualBlock((self.dim_mul* model_size) //8))\n        self.transformation = nn.Sequential(*transformation)\n\n        # Upsampling\n        stride = 4\n        if upsample:\n            stride = 1\n            upsample = 4\n        \n        deconv_layers = [\n            Transpose1dLayer( (self.dim_mul* model_size) //8,  (self.dim_mul* model_size) //16, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm),\n            #Transpose1dLayer( (self.dim_mul* model_size) //8,  (self.dim_mul* model_size) //16, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm),\n        ]\n        \n\n        if slice_len== 16384:\n            deconv_layers.append( Transpose1dLayer((self.dim_mul* model_size) //16, num_channels, 25, stride, upsample=upsample))\n        elif slice_len == 32768 :\n            deconv_layers +=[ \n                Transpose1dLayer((self.dim_mul* model_size) //16, model_size, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm)\n                ,Transpose1dLayer(model_size, num_channels, 25, 2, upsample=upsample)\n            ]\n        elif slice_len == 65536:\n            deconv_layers +=[\n                Transpose1dLayer((self.dim_mul* model_size) //16, model_size, 25, stride, upsample=upsample,use_batch_norm=use_batch_norm)\n                ,Transpose1dLayer(model_size, num_channels, 25, stride, upsample=upsample)\n            ]\n        else:\n            raise ValueError('slice_len {} value is not supported'.format(slice_len))\n        self.upsampling = nn.Sequential( *deconv_layers)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        if self.verbose:\n            print(x.shape)\n        x = self.transformation(x)\n        if self.verbose:\n            print(x.shape)\n        x = self.upsampling(x)\n        if self.verbose:\n            print(x.shape)\n        return x","metadata":{"id":"f4TD3PoHrDtn","execution":{"iopub.status.busy":"2024-03-02T07:16:15.531897Z","iopub.execute_input":"2024-03-02T07:16:15.532265Z","iopub.status.idle":"2024-03-02T07:16:15.551638Z","shell.execute_reply.started":"2024-03-02T07:16:15.532238Z","shell.execute_reply":"2024-03-02T07:16:15.550514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discriminator Model With Latent Code Eval","metadata":{"id":"G0BEGQGtQwCC"}},{"cell_type":"code","source":"class WaveGANDiscriminatorSep(nn.Module):\n    def __init__(self, model_size=64,  num_channels=1, shift_factor=2,\n                 alpha=0.2, verbose=False, slice_len=16384, use_batch_norm = False, drop_prob=0.2):\n        super(WaveGANDiscriminatorSep, self).__init__()\n        assert slice_len in [16384, 32768, 65536] # used to predict longer utterances\n        \n        self.verbose = verbose\n        self.model_size = model_size\n \n        conv_layers = [\n            Conv1D(num_channels, model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor, drop_prob = drop_prob),\n            Conv1D(model_size, 2 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor, drop_prob = drop_prob),\n            Conv1D(2 * model_size, 4 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor, drop_prob = drop_prob),\n        ]\n\n        if slice_len == 32768 :\n            conv_layers.append(\n                 Conv1D(4 * model_size, 4 * model_size, 25, stride=2, padding=12,use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n            ) \n        elif slice_len == 65536:\n            conv_layers.append(\n                 Conv1D(4 * model_size, 4 * model_size, 25, stride=4, padding=12,use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n            )\n        self.infer_input_audio = nn.ModuleList(conv_layers)\n\n        self.move_z_space = nn.Linear(noise_latent_dim, 64 * model_size)\n        self.infer_z = nn.ModuleList([\n            Conv1D( 1, model_size//8, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, drop_prob = drop_prob),\n            Conv1D(model_size//8, model_size//4, 25, stride=4, padding=11,use_batch_norm=use_batch_norm, drop_prob = drop_prob),\n        ])\n        \n        conv_joint_layers = [\n            Conv1D((4 * model_size)+(model_size//4), 8 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor, drop_prob = drop_prob),\n            Conv1D(8 * model_size, 16 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=0, drop_prob = 0 if slice_len==16384 else drop_prob)\n        ]\n        \n        self.fc_input_size = 8 * 32 * model_size\n        if slice_len == 32768 :\n            conv_joint_layers.append(\n                 Conv1D(16 * model_size, 32 * model_size, 25, stride=2, padding=11,use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n            ) \n            self.fc_input_size = 7 * 32 * model_size\n        elif slice_len == 65536:\n            conv_joint_layers.append(\n                 Conv1D(16 * model_size, 32 * model_size, 25, stride=4, padding=11,use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n            )\n            self.fc_input_size = 4 * 32 * model_size\n        self.infer_join = nn.ModuleList(conv_joint_layers)\n        self.fc1 = nn.Linear(self.fc_input_size, 1)\n        \n\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight.data)\n\n    def forward(self, x, z):\n        for conv in self.infer_input_audio:\n            x = conv(x)\n            if self.verbose:\n                print(x.shape)\n        if self.verbose:\n            print(x.shape)\n        z = self.move_z_space(z).view(-1, 1, self.model_size * 64)\n        if self.verbose:\n            print(z.shape)\n        for conv in self.infer_z:\n            z = conv(z)\n            if self.verbose:\n                print(z.shape)\n        \n        # join z and x\n        output = torch.cat([x, z], dim=1)\n        if self.verbose:\n            print(output.shape)\n        for conv in self.infer_join:\n            output = conv(output)\n            if self.verbose:\n                print(output.shape)\n        output = output.view(-1, self.fc_input_size)\n        if self.verbose:\n            print(output.shape)\n        return self.fc1(output)","metadata":{"id":"6ip7uE5jQ0Af","execution":{"iopub.status.busy":"2024-03-02T07:16:15.934063Z","iopub.execute_input":"2024-03-02T07:16:15.934428Z","iopub.status.idle":"2024-03-02T07:16:15.957324Z","shell.execute_reply.started":"2024-03-02T07:16:15.934401Z","shell.execute_reply":"2024-03-02T07:16:15.956390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Latent Space Encoder","metadata":{"id":"anx6LvECQ41-"}},{"cell_type":"code","source":"class WaveGanEncoder(nn.Module):\n    def __init__(self,model_size=64,  num_channels=1, shift_factor=2,\n                 alpha=0.2, verbose=False, slice_len=16384, use_batch_norm = False):\n        super(WaveGanEncoder, self).__init__()\n        self.verbose = verbose\n        conv_layers = [\n            Conv1D(num_channels, model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            Conv1D(model_size, 2 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            Conv1D(2 * model_size, 4 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n            Conv1D(4 * model_size, 8 * model_size, 25, stride=4, padding=11, use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor),\n        ]\n        self.fc_input_size = 8 * model_size * 16\n        if slice_len == 32768 :\n            conv_layers.append(\n                 Conv1D(8 * model_size, 16 * model_size, 25, stride=2, padding=11,use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n            ) \n            self.fc_input_size = 16 * model_size * 16\n        elif slice_len == 65536:\n            conv_layers.append(\n                 Conv1D(8 * model_size, 16 * model_size, 25, stride=4, padding=11,use_batch_norm=use_batch_norm, alpha=alpha,shift_factor=shift_factor)\n            )\n            self.fc_input_size = 16 * model_size * 16\n        self.conv_layers = nn.ModuleList(conv_layers)\n\n        self.adaptive_pooling = nn.AdaptiveAvgPool1d(16) # needs to be checked later\n\n        self.mu_linear = nn.Linear(self.fc_input_size, noise_latent_dim)\n        self.logvar_linear = nn.Linear(self.fc_input_size, noise_latent_dim)\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight.data)\n    \n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        z = mu + std * sample_noise(std.size(0))\n        return z\n\n    def bottleneck(self, h):\n        mu, logvar = self.mu_linear(h), self.logvar_linear(h)\n        z = self.reparametrize(mu, logvar)\n        return z, mu, logvar\n\n    def forward(self, x, inference=True):\n        for conv in self.conv_layers:\n            x= conv(x)\n            if self.verbose:\n                print(x.shape)\n        x = self.adaptive_pooling(x)\n        if self.verbose:\n            print(x.shape)\n        x = x.view(-1, self.fc_input_size)\n        if self.verbose:\n            print(x.shape)\n        z, _, _ = self.bottleneck(x)\n        return z","metadata":{"id":"ehfs42VsQ7l8","execution":{"iopub.status.busy":"2024-03-02T07:16:16.357252Z","iopub.execute_input":"2024-03-02T07:16:16.357949Z","iopub.status.idle":"2024-03-02T07:16:16.374647Z","shell.execute_reply.started":"2024-03-02T07:16:16.357900Z","shell.execute_reply":"2024-03-02T07:16:16.373709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Init and Testing","metadata":{"id":"JXDsLG4xHuIi"}},{"cell_type":"code","source":"z = sample_noise(10)\ngenerator_model = WaveGANGenerator(verbose=True, upsample=True,  use_batch_norm=False ,slice_len = window_length).to(device)\ngenerated_audio_test = generator_model(z)\nprint(generated_audio_test.shape)","metadata":{"id":"6IkIOCh_Hv09","outputId":"d6cce5a4-cfae-4cdf-9aee-1916fd8b065c","execution":{"iopub.status.busy":"2024-03-02T07:16:16.856032Z","iopub.execute_input":"2024-03-02T07:16:16.856394Z","iopub.status.idle":"2024-03-02T07:16:18.040878Z","shell.execute_reply.started":"2024-03-02T07:16:16.856368Z","shell.execute_reply":"2024-03-02T07:16:18.039973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_model.verbose= False\nsummary(generator_model, input_size=(1,noise_latent_dim))","metadata":{"id":"M_KXlWMWIEDQ","outputId":"88745063-6360-424b-b378-e6b4bec41c17","execution":{"iopub.status.busy":"2024-03-02T07:16:18.042813Z","iopub.execute_input":"2024-03-02T07:16:18.043180Z","iopub.status.idle":"2024-03-02T07:16:18.073434Z","shell.execute_reply.started":"2024-03-02T07:16:18.043146Z","shell.execute_reply":"2024-03-02T07:16:18.072534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator_model = WaveGANDiscriminator(verbose=True, use_batch_norm=True  ,slice_len = window_length).to(device)\ndiscriminating_output = discriminator_model(generated_audio_test)\nprint(discriminating_output.shape)","metadata":{"id":"3uWoLAztIIR5","outputId":"51d033a0-f6d3-40cc-a51a-6d9f8a3dea64","execution":{"iopub.status.busy":"2024-03-02T07:16:18.074662Z","iopub.execute_input":"2024-03-02T07:16:18.075046Z","iopub.status.idle":"2024-03-02T07:16:18.523918Z","shell.execute_reply.started":"2024-03-02T07:16:18.075014Z","shell.execute_reply":"2024-03-02T07:16:18.522912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator_model.verbose= False\nsummary(discriminator_model, input_size=(1,window_length))","metadata":{"id":"NQ67upDZIZqq","outputId":"8e0a2296-0568-4c06-8423-77e7ecc7030b","execution":{"iopub.status.busy":"2024-03-02T07:16:18.526299Z","iopub.execute_input":"2024-03-02T07:16:18.526658Z","iopub.status.idle":"2024-03-02T07:16:18.543743Z","shell.execute_reply.started":"2024-03-02T07:16:18.526626Z","shell.execute_reply":"2024-03-02T07:16:18.542913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator_model = WaveGANDiscriminatorSep(verbose=True  ,slice_len = window_length).to(device)\ndiscriminating_output = discriminator_model(generated_audio_test, sample_noise(10))\nprint(discriminating_output.shape)","metadata":{"id":"1qq1MUFeRKI-","outputId":"91960b91-873f-4197-9523-3c909d1ad39c","execution":{"iopub.status.busy":"2024-03-02T07:16:18.544729Z","iopub.execute_input":"2024-03-02T07:16:18.544999Z","iopub.status.idle":"2024-03-02T07:16:18.968783Z","shell.execute_reply.started":"2024-03-02T07:16:18.544972Z","shell.execute_reply":"2024-03-02T07:16:18.967573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator_model.verbose=False\nsummary(discriminator_model, input_size=[(1,window_length),(1,noise_latent_dim)])","metadata":{"id":"FnL_bKItRjUr","outputId":"dfc0bc0e-6800-4c34-cd87-b07dc6787f72","execution":{"iopub.status.busy":"2024-03-02T07:16:18.969864Z","iopub.execute_input":"2024-03-02T07:16:18.970152Z","iopub.status.idle":"2024-03-02T07:16:18.984539Z","shell.execute_reply.started":"2024-03-02T07:16:18.970127Z","shell.execute_reply":"2024-03-02T07:16:18.983705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_model = WaveGanEncoder(verbose=True  ,slice_len = window_length).to(device)\nlatent_encoded = encoder_model(generated_audio_test)\nprint(latent_encoded.shape)","metadata":{"id":"jkSypp8RRKUZ","outputId":"c7a6b99d-f493-45cc-9429-111cfa14d29f","execution":{"iopub.status.busy":"2024-03-02T07:16:18.986330Z","iopub.execute_input":"2024-03-02T07:16:18.986598Z","iopub.status.idle":"2024-03-02T07:16:19.151210Z","shell.execute_reply.started":"2024-03-02T07:16:18.986570Z","shell.execute_reply":"2024-03-02T07:16:19.150316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_model.verbose=False\nsummary(encoder_model, input_size=(1,window_length))","metadata":{"id":"c9DIl5VxRqgk","outputId":"72ca9a6e-5627-44e8-8302-97a611cbbaac","execution":{"iopub.status.busy":"2024-03-02T07:16:19.152421Z","iopub.execute_input":"2024-03-02T07:16:19.152764Z","iopub.status.idle":"2024-03-02T07:16:19.161716Z","shell.execute_reply.started":"2024-03-02T07:16:19.152731Z","shell.execute_reply":"2024-03-02T07:16:19.160831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cyclic_generator = CyclicWaveGanGenerator(slice_len=window_length, verbose = True).to(device)\ngenerated = cyclic_generator(Variable(torch.rand(10,1,window_length)).to(device))\nprint(generated.shape)","metadata":{"id":"XUPONVxSrOF1","outputId":"8dd20e3c-393e-44d4-aef4-c0e7848f79b0","execution":{"iopub.status.busy":"2024-03-02T07:16:19.162781Z","iopub.execute_input":"2024-03-02T07:16:19.163109Z","iopub.status.idle":"2024-03-02T07:16:19.255205Z","shell.execute_reply.started":"2024-03-02T07:16:19.163078Z","shell.execute_reply":"2024-03-02T07:16:19.254330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cyclic_generator.verbose= False\nsummary(cyclic_generator, input_size=(1,window_length))","metadata":{"id":"VL0-X0FAraW7","outputId":"84f5a380-0cb9-4c8c-c27e-527bb3abab63","execution":{"iopub.status.busy":"2024-03-02T07:16:19.256884Z","iopub.execute_input":"2024-03-02T07:16:19.257182Z","iopub.status.idle":"2024-03-02T07:16:19.308771Z","shell.execute_reply.started":"2024-03-02T07:16:19.257158Z","shell.execute_reply":"2024-03-02T07:16:19.307970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cyclic Gan","metadata":{"id":"NqLXp66Qq2mI"}},{"cell_type":"code","source":"val_set = iter(train_loader)\nprint(val_set)\nval_data = next(val_set)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:16:19.651185Z","iopub.execute_input":"2024-03-02T07:16:19.651508Z","iopub.status.idle":"2024-03-02T07:16:19.766783Z","shell.execute_reply.started":"2024-03-02T07:16:19.651483Z","shell.execute_reply":"2024-03-02T07:16:19.765191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CycleGan(object):\n    def __init__(self, train_loader, val_loader, validate=True):\n        # train cyclic to just disable the model from single source to mixed as an experiment\n\n        self.validate = validate\n\n        self.val_g_cost = []\n        self.train_g_cost = []\n        self.cyclic_loss = []\n        self.valid_reconstruction = []\n\n        self.discriminator_loss = []\n\n        self.generator = CyclicWaveGanGenerator(slice_len = window_length, model_size = model_capacity_size).to(device)\n        self.generator.apply(weights_init)\n        \n        self.discriminator_1 = WaveGANDiscriminator(slice_len = window_length, model_size = model_capacity_size).to(device)\n        self.discriminator_1.apply(weights_init)\n    \n        self.discriminator_2 = WaveGANDiscriminator(slice_len = window_length, model_size = model_capacity_size).to(device)\n        self.discriminator_2.apply(weights_init)\n\n\n        self.optimizer_d_1 = optim.Adam(self.discriminator_1.parameters(), lr=lr_d, betas=(beta1,  beta2))\n        self.optimizer_d_2 = optim.Adam(self.discriminator_2.parameters(), lr=lr_d, betas=(beta1,  beta2))\n        \n\n        \n\n        self.optimizer_g = optim.Adam(self.generator.parameters(), lr=lr_g, betas=(beta1,  beta2))\n\n        self.train_loader = train_loader\n        self.val_loader = val_loader          \n    \n    def apply_zero_grad(self):\n        self.discriminator_1.zero_grad()\n        self.discriminator_2.zero_grad()\n        self.generator.zero_grad()\n\n    def enable_gen_disable_disc(self):\n        gradients_status(self.generator, True)\n        gradients_status(self.discriminator_1, False)\n        gradients_status(self.discriminator_2, False)\n    \n    def disable_all(self):\n        gradients_status(self.generator, False)\n        gradients_status(self.discriminator_1, False)\n        gradients_status(self.discriminator_2, False)\n    \n  \n    def train(self):\n        real_label = 0.9\n        progress_bar =  tqdm(total=n_iterations//progress_bar_step_iter_size)\n        val_set = iter(self.val_loader)\n        val_data = next(val_set)\n        fixed_mixed_signal = val_data[1].to(device)\n        fixed_single_signal = val_data[0].to(device)\n        save_samples(fixed_mixed_signal.detach().cpu().numpy(), 'fixed_mixed' ) \n        save_samples(fixed_single_signal.detach().cpu().numpy(), 'fixed_single' ) \n        \n        gan_model_name = 'gan_cyclic_single_2disc_{}.tar'.format(model_prefix)  #.tar\n        \n        first_iter = 0\n        if take_backup and os.path.isfile(gan_model_name):\n            if cuda:\n                checkpoint = torch.load(gan_model_name)\n            else:\n                checkpoint = torch.load(gan_model_name, map_location='cpu')\n            self.generator.load_state_dict(checkpoint['generator'])\n            self.discriminator_1.load_state_dict(checkpoint['discriminator_1'])\n            self.discriminator_2.load_state_dict(checkpoint['discriminator_2'])\n            self.optimizer_d_1.load_state_dict(checkpoint['optimizer_d_1'])\n            self.optimizer_d_2.load_state_dict(checkpoint['optimizer_d_2'])\n            self.optimizer_g.load_state_dict(checkpoint['optimizer_g'])\n            self.val_g_cost = checkpoint['val_g_cost']\n            self.train_g_cost = checkpoint['train_g_cost']\n            self.cyclic_loss = checkpoint['cyclic_loss']\n            self.discriminator_loss = checkpoint['discriminator_loss']\n            first_iter = checkpoint['n_iterations'] + 1\n            for _ in range(0, first_iter, progress_bar_step_iter_size):\n                progress_bar.update()\n            \n            \n            self.generator.eval()\n        criterion_GAN = nn.MSELoss()\n        criterion_cycle = nn.L1Loss()\n        criterion_identity = nn.L1Loss()\n\n        target_real = Variable(torch.Tensor(batch_size,1).fill_(1.0), requires_grad=False).to(device)\n        target_fake = Variable(torch.Tensor(batch_size,1).fill_(0.0), requires_grad=False).to(device)\n        \n        generated_single_buffer = ReplayBuffer()\n        train_set = iter(self.train_loader)\n        for iter_indx in range(first_iter, n_iterations): \n            self.generator.train()\n            self.discriminator_1.train()\n            self.discriminator_2.train()\n            try:\n                data = next(train_set)\n            except StopIteration:\n                train_set = iter(self.train_loader)\n                data = next(train_set)\n            \n            # in case of unpaired data\n            single_signal = data[0].to(device)\n            mixed_signal = data[1].to(device)\n\n\n            #############################\n            # Training First Discriminator\n            #############################\n            self.apply_zero_grad()\n            self.disable_all()\n            gradients_status(self.discriminator_1, True)\n            generated_single_signal = self.generator(single_signal)\n            rest_of_signal = mixed_signal - generated_single_signal\n            new_mixed_signal = rest_of_signal + single_signal\n\n            # Real loss\n            is_single_signal_r = self.discriminator_1(single_signal)\n            #d_loss_real_1 = criterion_GAN(is_single_signal_r,target_real)\n\n            # generated loss\n            #generated_single_signal = generated_single_buffer.push_and_pop(generated_single_signal)\n            is_single_signal_f = self.discriminator_1(generated_single_signal.detach())\n            #d_loss_generated_1 = criterion_GAN(is_single_signal_f, target_fake)\n\n            d_loss_1 = (torch.mean((is_single_signal_r - torch.mean(is_single_signal_f) - target_real) ** 2) +\n                torch.mean((is_single_signal_f - torch.mean(is_single_signal_r) + target_real) ** 2))/2#(d_loss_real_1 + d_loss_generated_1)/2\n            d_loss_1.backward()\n            self.optimizer_d_1.step()\n            #############################\n            # Training Second Discriminator\n            #############################\n            self.apply_zero_grad()\n            self.disable_all()\n            gradients_status(self.discriminator_2, True)\n            # Real loss\n            is_mixed_signal_r = self.discriminator_2(mixed_signal)\n            #d_loss_real_1 = criterion_GAN(is_mixed_signal_r,target_real)\n\n            # generated loss\n            #generated_single_signal = generated_single_buffer.push_and_pop(generated_single_signal)\n            is_mixed_signal_f = self.discriminator_2(new_mixed_signal.detach())\n            #d_loss_generated_1 = criterion_GAN(is_mixed_signal_f, target_fake)\n\n            d_loss_2 = (torch.mean((is_mixed_signal_r - torch.mean(is_mixed_signal_f) - target_real) ** 2) +\n                torch.mean((is_mixed_signal_f - torch.mean(is_mixed_signal_r) + target_real) ** 2))/2#(d_loss_real_1 + d_loss_generated_1)/2\n            d_loss_2.backward()\n            self.optimizer_d_2.step()\n\n            #############################\n            # Training  generator\n            #############################\n            self.apply_zero_grad()\n            self.enable_gen_disable_disc()\n            \n\n            # Identity loss without it the model would make changes to input even without any need\n\n            \n            identity_loss_1 = criterion_identity(generated_single_signal, single_signal)\n        \n            # Gan Loss\n            #generated_single_signal = self.generator(mixed_signal)\n            is_single_signal_r = self.discriminator_1(single_signal)\n            is_single_signal_f = self.discriminator_1(generated_single_signal)\n            gan_loss_1 = (torch.mean((is_single_signal_r - torch.mean(is_single_signal_f) + target_real) ** 2) +\n                torch.mean((is_single_signal_f - torch.mean(is_single_signal_r) - target_real) ** 2))/2\n\n            #gan_loss_1 =criterion_GAN(is_single_signal, target_real )\n\n\n            is_mixed_signal_r = self.discriminator_2(mixed_signal)\n            is_mixed_signal_f = self.discriminator_2(new_mixed_signal)\n            gan_loss_2 = (torch.mean((is_mixed_signal_r - torch.mean(is_mixed_signal_f) + target_real) ** 2) +\n                torch.mean((is_mixed_signal_f - torch.mean(is_mixed_signal_r) - target_real) ** 2))/2\n\n            #gan_loss_2 = criterion_GAN(is_mixed_signal, target_real )\n\n            reconstructed_single_sinal = self.generator(new_mixed_signal)\n\n            cycle_loss_1 = criterion_cycle(reconstructed_single_sinal, single_signal)\n            # Total Loss\n            g_cost  =  identity_loss_1*0.5 + (gan_loss_1 + gan_loss_2)  + 10*cycle_loss_1 \n            g_cost.backward()\n            self.optimizer_g.step()\n\n\n            if self.validate and iter_indx%store_cost_every==0:\n                self.discriminator_loss.append(d_loss_1.item())\n                self.train_g_cost.append(g_cost.item())\n                # validating\n                self.disable_all()\n                with torch.no_grad():\n                    try:\n                        val_data = next(val_set)\n                    except StopIteration:\n                        val_set = iter(self.val_loader)\n                        val_data = next(val_set)\n                    val_single = val_data[0].to(device)\n                    val_mixed = val_data[1].to(device)\n                    val_cost= criterion_GAN(self.discriminator_1(val_single), target_real) + criterion_GAN(self.discriminator_2(val_single), target_real)\n                    self.val_g_cost.append(val_cost.item())\n                    reconstructed_music = self.generator(val_mixed)\n                    self.valid_reconstruction.append(F.mse_loss(reconstructed_music, val_single, reduction='sum').item())\n                    \n\n            if iter_indx%store_cost_every==0 :\n                progress_updates = {'Reconstruction': str(self.valid_reconstruction[-1]),'Loss_D1': str(d_loss_1.item()), 'Loss_g':str(g_cost.item())}\n                progress_bar.set_postfix(progress_updates)\n\n\n            if iter_indx%progress_bar_step_iter_size==0:\n                progress_bar.update()\n            # lr decay \n            if decay_lr:\n                decay = max(0.0,1.0-(iter_indx*1.0/n_iterations))\n                # update the learning rate\n                update_optimizer_lr(self.optimizer_d, lr_d, decay)\n                update_optimizer_lr(self.optimizer_g, lr_g, decay)\n\n            if (iter_indx%save_samples_every==0):\n                with torch.no_grad():\n                    fake = self.generator(fixed_mixed_signal).detach().cpu().numpy()\n                save_samples(fake, iter_indx , prefix='predictions')\n            \n            if take_backup and iter_indx%backup_every_n_iters==0:\n                saving_dict = {\n                    'generator': self.generator.state_dict(),\n                    'discriminator_1': self.discriminator_1.state_dict(),\n                    'optimizer_d_1': self.optimizer_d_1.state_dict(), \n                    'discriminator_2': self.discriminator_2.state_dict(),\n                    'optimizer_d_2': self.optimizer_d_2.state_dict(), \n                    'optimizer_g': self.optimizer_g.state_dict(),\n                    'val_g_cost': self.val_g_cost,\n                    'train_g_cost': self.train_g_cost,\n                    'cyclic_loss': self.cyclic_loss,\n                    'discriminator_loss': self.discriminator_loss,\n                    'n_iterations': iter_indx\n                }\n                torch.save(saving_dict,gan_model_name)","metadata":{"id":"qvy6nHfUq4KA","execution":{"iopub.status.busy":"2024-03-02T07:16:19.898523Z","iopub.execute_input":"2024-03-02T07:16:19.898916Z","iopub.status.idle":"2024-03-02T07:16:19.944874Z","shell.execute_reply.started":"2024-03-02T07:16:19.898878Z","shell.execute_reply":"2024-03-02T07:16:19.943971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:16:20.108525Z","iopub.execute_input":"2024-03-02T07:16:20.108853Z","iopub.status.idle":"2024-03-02T07:16:20.113048Z","shell.execute_reply.started":"2024-03-02T07:16:20.108827Z","shell.execute_reply":"2024-03-02T07:16:20.112130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{"id":"Cz3XfTqIJRBx"}},{"cell_type":"code","source":"model = CycleGan(train_loader, val_loader)\ngan_model_name = '/kaggle/input/test-bansuri-checkpoint/gan_cyclic_single_2disc_exp_musdb_1_wide_unpaired_ralsgan_4.tar'.format(model_prefix)\ncheckpoint = torch.load(gan_model_name, map_location='cpu')\n\n\nmodel.generator.load_state_dict(checkpoint['generator'])\nmodel.discriminator_1.load_state_dict(checkpoint['discriminator_1'])\nmodel.discriminator_2.load_state_dict(checkpoint['discriminator_2'])\nmodel.optimizer_d_1.load_state_dict(checkpoint['optimizer_d_1'])\nmodel.optimizer_d_2.load_state_dict(checkpoint['optimizer_d_2'])\nmodel.optimizer_g.load_state_dict(checkpoint['optimizer_g'])\n\n\nmodel.val_g_cost = checkpoint['val_g_cost']\nmodel.train_g_cost = checkpoint['train_g_cost']\nmodel.cyclic_loss = checkpoint['cyclic_loss']\nmodel.discriminator_loss = checkpoint['discriminator_loss']\nn_iterations = checkpoint['n_iterations']\n\ngan_model = model","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:16:20.552528Z","iopub.execute_input":"2024-03-02T07:16:20.553164Z","iopub.status.idle":"2024-03-02T07:16:27.847773Z","shell.execute_reply.started":"2024-03-02T07:16:20.553132Z","shell.execute_reply":"2024-03-02T07:16:27.846983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gan_model = CycleGan(train_loader,val_loader)\ngan_model.train()","metadata":{"id":"_ObmMvYCJSDV","outputId":"2af85b3f-6133-485a-cb8d-f17c4aec8241","execution":{"iopub.status.busy":"2024-03-02T07:17:42.745243Z","iopub.execute_input":"2024-03-02T07:17:42.745576Z","iopub.status.idle":"2024-03-02T07:21:57.118820Z","shell.execute_reply.started":"2024-03-02T07:17:42.745552Z","shell.execute_reply":"2024-03-02T07:21:57.117735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluation**","metadata":{}},{"cell_type":"code","source":"!pip install mir_eval\n!pip install museval","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:21:57.121087Z","iopub.execute_input":"2024-03-02T07:21:57.121389Z","iopub.status.idle":"2024-03-02T07:22:24.206487Z","shell.execute_reply.started":"2024-03-02T07:21:57.121361Z","shell.execute_reply":"2024-03-02T07:22:24.205303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Results_vocal_MUSDB - SIR 11.670795746936527 SAR 11.670795746936527 SDR 6.5125854585956136 ----1000000-iter\n## REsults-vocal_nepali - SIR 11.603162618823603 SAR 11.603162618823603 SDR 7.324119863550323 ---- 100000 -iter\nimport mir_eval\nimport museval\nimport gc\ndef _any_source_silent(sources):\n    \"\"\"Returns true if the parameter sources has any silent first dimensions\"\"\"\n    return np.any(np.all(np.sum(\n        sources, axis=tuple(range(2, sources.ndim))) == 0, axis=1))\n\ntest_dataset = LMDBWavLoader('musdb_valid',True)   ## LMDBWavLoader('/kaggle/input/musdb-valid/musdb_valid/musdb_valid', True) \ntest_data_len = len(test_dataset) \neps = 1e-15\nwith torch.no_grad():\n    # to get 1 min output\n    reconstructed_signals = []\n    mixed_signals = []\n    original_signals = []\n    out_sdr = []\n    out_sir = []\n    out_sar = []\n    for i in tqdm(range(test_data_len)):\n        #sample_rate = 20480\n        val_data  = test_dataset[i] \n        mixed_signal = []\n        wav_iter = list(audio_generator(val_data[1]))\n        subsample_n_samples = len(wav_iter)\n        for j in range(subsample_n_samples):\n            mixed_signal.append(wav_iter[j])\n        mixed_signal = torch.squeeze(torch.from_numpy(np.stack(mixed_signal,axis=0)).float()).to(device)\n        mixed_signal = torch.unsqueeze(mixed_signal, dim=1)\n        reconstructed = gan_model.generator(mixed_signal) \n        single_source = val_data[0]\n        \n        n_items_per_eval = 3\n        data_len = (single_source.shape[-1]//n_items_per_eval) * n_items_per_eval\n        reconstructed = torch.squeeze(reconstructed).detach().cpu().numpy().reshape(1,-1)[:,:data_len]\n\n        reconstructed = reconstructed[:data_len].reshape( -1)\n        single_source = single_source[:data_len].reshape( -1) \n        mixed_source = val_data[1][:data_len].reshape( -1)\n        clean_inference = mixed_source - single_source\n        predicted_inference = mixed_source - reconstructed\n        #single_source = librosa.resample(single_source, sample_rate, window_length) \n        #reconstructed = librosa.resample(reconstructed, sample_rate, window_length)\n        reference_music =single_source#, mixed_signal[:data_len].reshape(n_items_per_eval, -1) - single_source[:data_len].reshape(n_items_per_eval, -1)])\n        estimates_music = reconstructed#, mixed_signal[:data_len].reshape(n_items_per_eval, -1) - reconstructed])\n        del reconstructed\n        del single_source\n        del mixed_signal\n        gc.collect()\n        sdr_b,  sir_b, sar_b, _ =mir_eval.separation.bss_eval_sources_framewise(np.array([reference_music, clean_inference]), np.array([estimates_music, predicted_inference]))\n        sdr, sir, sar = sdr_b, sir_b, sar_b\n        #sdr_inter, sir_inter, sar_inter = sdr_b[1], sir_b[1], sar_b[1]\n        out_sdr.append(np.mean(sdr[~np.isnan(sdr)]))\n        out_sir.append(np.mean(sir[~np.isnan(sir)]))\n        out_sar.append(np.mean(sir[~np.isnan(sar)]))\n    sdr  = np.median(out_sdr)\n    sir = np.median(out_sir)\n    sar = np.median(out_sar)\n    print('SIR {} SAR {} SDR {}'.format(str(sir),str(sar),str(sdr)))","metadata":{"id":"gyThAcWdtrIk","execution":{"iopub.status.busy":"2024-03-02T07:22:24.208477Z","iopub.execute_input":"2024-03-02T07:22:24.208796Z","iopub.status.idle":"2024-03-02T07:22:36.779860Z","shell.execute_reply.started":"2024-03-02T07:22:24.208768Z","shell.execute_reply":"2024-03-02T07:22:36.778979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Audio\n\ndef preprocess(x):\n    output_array = np.copy(x)\n    audio_len = output_array.shape[-1]\n    for i in range(window_length, audio_len, window_length):\n        output_array[0,0,i-1] = 0\n    return output_array\n    \ndef predict_long_wav(wav_file_path, out_folder_name):\n    print(wav_file_path)\n    wav_data = load_wav(wav_file_path)\n    wav_iter = list(audio_generator(wav_data))\n    wav_iter = np.stack(wav_iter)\n    wav_iter = torch.from_numpy(wav_iter).float().to(device)\n    wav_iter = torch.unsqueeze(wav_iter,dim=1)\n    reconstructed = gan_model.generator(wav_iter)\n    x = reconstructed.view(1,1,-1).detach().cpu().numpy()\n    x = preprocess(x)\n    save_samples(x,  out_folder_name )\n    return x\nx = predict_long_wav('/kaggle/input/nepali-music-source-seperation/nepali_music_source_seperation/train/mixture/01_mixture.wav', 'out')","metadata":{"id":"PwITOkhsSPgD","execution":{"iopub.status.busy":"2024-03-02T07:22:36.781707Z","iopub.execute_input":"2024-03-02T07:22:36.782013Z","iopub.status.idle":"2024-03-02T07:22:36.913959Z","shell.execute_reply.started":"2024-03-02T07:22:36.781986Z","shell.execute_reply":"2024-03-02T07:22:36.913054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(x[0][0], rate=16000)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:22:36.914957Z","iopub.execute_input":"2024-03-02T07:22:36.915220Z","iopub.status.idle":"2024-03-02T07:22:36.941998Z","shell.execute_reply.started":"2024-03-02T07:22:36.915196Z","shell.execute_reply":"2024-03-02T07:22:36.940888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## for testing the output of the model - storing the testing results\n# for file in range(120, 150, 1):\n#     predict_long_wav(f'/kaggle/input/to-delete-renamed/Newari_song/Newari_song/mixture/mixture_{file}.wav', f'out{file}')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T12:55:24.284550Z","iopub.status.idle":"2024-02-26T12:55:24.284915Z","shell.execute_reply.started":"2024-02-26T12:55:24.284750Z","shell.execute_reply":"2024-02-26T12:55:24.284765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Mi8SaE4_9r-1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}